{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DSP performance prediction",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BrandonCay/-DSP-performance-prediction/blob/main/DSP_performance_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fCEDCU_qrC0"
      },
      "source": [
        "<p><img alt=\"Colaboratory logo\" height=\"45px\" src=\"/img/colab_favicon.ico\" align=\"left\" hspace=\"10px\" vspace=\"0px\"></p>\n",
        "\n",
        "<h1>What is Colaboratory?</h1>\n",
        "\n",
        "Colaboratory, or \"Colab\" for short, allows you to write and execute Python in your browser, with \n",
        "- Zero configuration required\n",
        "- Free access to GPUs\n",
        "- Easy sharing\n",
        "\n",
        "Whether you're a **student**, a **data scientist** or an **AI researcher**, Colab can make your work easier. Watch [Introduction to Colab](https://www.youtube.com/watch?v=inN8seMm7UI) to learn more, or just get started below!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIyqFyP4YiBT"
      },
      "source": [
        ""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJBs_flRovLc"
      },
      "source": [
        "## **Getting started**\n",
        "\n",
        "The document you are reading is not a static web page, but an interactive environment called a **Colab notebook** that lets you write and execute code.\n",
        "\n",
        "For example, here is a **code cell** with a short Python script that computes a value, stores it in a variable, and prints the result:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJr_9dXGpJ05",
        "outputId": "579ba6d4-29fc-4bd0-9ef7-108f4d408767"
      },
      "source": [
        "seconds_in_a_day = 24 * 60 * 60\n",
        "seconds_in_a_day"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "86400"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fhs6GZ4qFMx"
      },
      "source": [
        "To execute the code in the above cell, select it with a click and then either press the play button to the left of the code, or use the keyboard shortcut \"Command/Ctrl+Enter\". To edit the code, just click the cell and start editing.\n",
        "\n",
        "Variables that you define in one cell can later be used in other cells:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gE-Ez1qtyIA",
        "outputId": "945a2f0b-e233-4686-ec38-c5253574db90"
      },
      "source": [
        "seconds_in_a_week = 7 * seconds_in_a_day\n",
        "seconds_in_a_week"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "604800"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSrWNr3MuFUS"
      },
      "source": [
        "Colab notebooks allow you to combine **executable code** and **rich text** in a single document, along with **images**, **HTML**, **LaTeX** and more. When you create your own Colab notebooks, they are stored in your Google Drive account. You can easily share your Colab notebooks with co-workers or friends, allowing them to comment on your notebooks or even edit them. To learn more, see [Overview of Colab](/notebooks/basic_features_overview.ipynb). To create a new Colab notebook you can use the File menu above, or use the following link: [create a new Colab notebook](http://colab.research.google.com#create=true).\n",
        "\n",
        "Colab notebooks are Jupyter notebooks that are hosted by Colab. To learn more about the Jupyter project, see [jupyter.org](https://www.jupyter.org)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdRyKR44dcNI"
      },
      "source": [
        "## Data science\n",
        "\n",
        "With Colab you can harness the full power of popular Python libraries to analyze and visualize data. The code cell below uses **numpy** to generate some random data, and uses **matplotlib** to visualize it. To edit the code, just click the cell and start editing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "C4HZx7Gndbrh",
        "outputId": "72dd9b97-04c6-457c-c41f-c31aeea4bc40"
      },
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "ys = 200 + np.random.randn(100)\n",
        "x = [x for x in range(len(ys))]\n",
        "\n",
        "plt.plot(x, ys, '-')\n",
        "plt.fill_between(x, ys, 195, where=(ys > 195), facecolor='g', alpha=0.6)\n",
        "\n",
        "plt.title(\"Sample Visualization\")\n",
        "plt.show()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOy9abQk11Um+u0YM+9Uk6rKpYmyLcmybOMBAX4P6DYYdzM+uxcNDQ+MAYPfa/y6bZ7htaGZ6abttcBgaCYP2DIWYBoJPOBJli1bsgarVC6VVKpSDarh3lt3vjnHHHHejxMnpozMjLw38w7l+NaqdbMiIyNPREbss8+39/42McZQokSJEiWuLUjbPYASJUqUKDF6lMa9RIkSJa5BlMa9RIkSJa5BlMa9RIkSJa5BlMa9RIkSJa5BlMa9RIkSJa5BlMa9xK4DEf02EX10TMc+RUSvGcexE9/BiOiW8PVfEtFvjOE7PkNEbxr1cUvsHpTGvURhENF3EtHDRNQgonUi+ioRfet2j6soiOizRPS7OdtfT0SLRKQwxl7CGHtgq8bEGPu/GWO/t5lj5E12jLHvZ4zdtbnRldjNKI17iUIgohkAnwLwpwD2A7gBwO8AsLdzXEPiLgA/RUSU2f5GAHczxrxtGFOJEmNBadxLFMVtAMAY+zvGmM8YMxljn2eMnQQAInohEX2RiNaIaJWI7iaiveLDRHSJiH6FiE4SUYeIPkhEh0P6oEVEXyCifeG+R0Pq4i1EdJWIFojol3sNjIheHa4o6kT0ZB9a5Z8BHADwXYnP7gPwQwA+khjn94avv42IjhFRk4iWiOg94fbXENFcZgzZzz0SjmeBiP4nEWk9xv5hIvpv4etPElE78S8gop8J33svEc2GY3mCiL4r3P59AH4NwH8IP/NkuP0BIvr58LVERL9ORJeJaJmIPkJEezLX+k1EdCX87f5rr2tdYvegNO4liuIsAJ+I7iKi7xeGOAEC8D8AXA/gxQBuAvDbmX1+BMDrwCeKHwbwGXDDdBD8XvzPmf2/G8CtAP4NgP8ijGfqS4luAPAvAP4b+IrilwHcQ0QHs/syxkwA/wDgpxObfwzAGcbYkznn/F4A72WMzQB4YfjZIvAB/BKA6wD8bwBeC+AXB32IMfbDjLEpxtgUgB8FsAjg/vDtxwG8Avwc/xbA/yKiCmPsswB+H8DHws++POfQPxP++24ALwAwBeB/Zvb5TgAvCsf6m0T04oLnWmKHojTuJQqBMdYENwAMwPsBrBDRJ4jocPj+ecbYfYwxmzG2AuA9AP515jB/yhhbYozNA3gQwGOMsa8zxiwA/wTglZn9f4cx1mGMPQXgQwB+ImdoPwXg04yxTzPGAsbYfQCOAfiBHqdyF4B/T0SV8P8/HW7LgwvgFiK6jjHWZow92mO/FBhjTzDGHmWMeYyxSwD+Ct3XoieI6LZwTD/GGJsNj/lRxthaeMw/BKCDG+Mi+EkA72GMPccYawP4VQA/TkRKYp/fCVdjTwJ4EkDeJFFiF6E07iUKgzF2mjH2M4yxGwG8FNxL/2MACCmWvyeieSJqAvgouOeaxFLitZnz/6nM/rOJ15fD78vimwD8aEiB1ImoDj4JHelxDg8BWAXwBiJ6IYBvA/eE8/Bm8FXGGSJ6nIh+qMd+KRDRbUT0qTBI2wT3rLPXotdn9wD4OIBfD8cqtv8yEZ0Og9l1AHuKHhP8ul1O/P8yAAXA4cS2xcRrA92/RYldhtK4l9gQGGNnAHwY3MgD3IAxAC8LaYyfAqdqNoObEq9vBnA1Z59ZAH/DGNub+DfJGHtXn+N+BNxj/ykAn2OMLeXtxBg7xxj7CQCHALwbwD8S0SSADoAJsR8RyeDUksBfADgD4NbwWvwaClwLIpLAJ5ovMcbel9j+XQD+P3AKaR9jbC+ARuKYg6Rdr4JPggI3A/CQnlxLXGMojXuJQiCi24noHUR0Y/j/m8BpEkFVTANoA2iEPPivjOBrf4OIJojoJQB+FsDHcvb5KIAfJqJ/S0QyEVXCgOeNfY77EQDfC+AX0JuSARH9FBEdZIwFAOrh5gA8/lAhoh8kIhXAr4PTJALTAJoA2kR0O4D/WOx08d8BTAJ4W2b7NLgxXgGgENFvAphJvL8E4Gg4OeTh7wD8EhE9n4imEHP0ZXbQNYzSuJcoihaAbwfwGBF1wI360wDeEb7/OwBeBe5R/guAe0fwnV8GcB48qPgHjLHPZ3cIOenXg3vHK+Ce/K+gz70d8uAPgxvST/T5/u8DcIqI2uDB1R8PeekGeID0AwDmwT35ZPbMLwP4P8Gv2fuRPynl4ScAvBpALZEx85MAPgfgs+CTymUAFtKU1f8K/64R0fGc4/41gL8B8BUAF8PP/6eCYyqxS0Fls44SOw1EdBTcCKmld1mixMZQeu4lSpQocQ2iNO4lSpQocQ2ipGVKlChR4hpE6bmXKFGixDUIZfAu48d1113Hjh49ut3DKFGiRIldhSeeeGKVMdYltQHsEON+9OhRHDt2bLuHUaJEiRK7CkR0udd7JS1TokSJEtcgSuNeokSJEtcgSuNeokSJEtcgSuNeokSJEtcgSuNeokSJEtcgBhp3IrqJiL5ERM8Q7wz/tnD7fiK6j4jOhX9Fi7TXE2+ldiJsUfad4z6JEiVKlCiRRhHP3QPwDsbYHeCKdW8lojsAvBPA/YyxW8FV+94Z7n8/gJczxl4B4OfAlfNKlChRosQWYqBxZ4wtMMaOh69bAE6Dd75/PWIt7LsAvCHcp81iTYNJDG4kUKJEiW9ANEwXHz8xv93DuGYxFOceSrG+EsBjAA4zxhbCtxaRaNlFRP+OiM6A63r/XI9jvSWkbY6trKxsYOglSpTYzbjniTm87e9PYKVlb/dQrkkUNu5hB5d7ALw9bJYcIfTUWeL//8QYux3cm/+9vOMxxt7HGLuTMXbnwYO51bMlSpS4hnG1bgIAbM/f5pFcmyhk3MNWYvcAuJsxJjrsLBHRkfD9IwCWs59jjH0FwAuIqGgj3xIlSnyDYLFpAQBcv2Rux4Ei2TIE4IMATjPG3pN46xMA3hS+fhN4x3YQ0S3hZ0BErwLvLbk2ykGXKFFi92OhwT13zw+2eSTXJooIh30HgDcCeIqIToTbfg3AuwD8AxG9Gbyv44+F7/0IgJ8mIheACeA/sFI0vkSJEhkI4+6Uxn0sGGjcGWMPAaAeb782Z/93A3j3JsdVokSJaxhBwLDc5IHUkpYZD8oK1RIlSmw51g0HXuiwu6XnPhaUxr1EiRJbjsWGFb0ujft4UBr3EiMBYwxlaKVEUaSNe3nfjAOlcS8xEvzshx7H733q9HYPo8QuwUIzYdy90nMfB3ZEm70Sux/nV1rwS8+9REEsJTx3LyiN+zhQeu4lRgLb89G03O0eRoldgoWEcXdKWmYsKI17iZHAcgO0SuNeoiAWGyY0NUyFLGmZsaA07iVGAtsrjXuJ4rjaMFDVOwDKbJlxoTTuJTYNxhgcj6FtlwJQJYphsWmjWjEAAG5Q0jLjQGncS2wadrisNp0AfvmglhiAluXCdILYuJe0zFhQGvcSm4btxg9n2/K2cSQldgNEjntVD417ScuMBaVxL7FpWAk97pY9ft49CBj+4oELaJglx78bIaR+J0LP3StXe2NBadxLbBqWmzDuW+C5n1ls4d2fPYMvnelqIVBiF2Ah47k7JS0zFpTGvcSmYSVoma0w7nXDAQCYbhnA3Y0QBUy6bkEiVtIyY0Jp3EtsGknPvb0FtEw9pGMMpzTuuxELTQu66kKWAkhSUBr3MaFIJ6abiOhLRPQMEZ0ioreF2/cT0X1EdC78uy/c/pNEdJKIniKih4no5eM+iRLbC9vbas+dG3er9Nx3JRYbFio6b9TBPfeScx8HinjuHoB3MMbuAPBqAG8lojsAvBPA/YyxWwHcH/4fAC4C+NeMsZeBN8d+3+iHXWInIWlkm1tg3GuClik9912JhYYBTeV8uySVtMy4MNC4M8YWGGPHw9ctAKcB3ADg9QDuCne7C8Abwn0eZozVwu2PArhx1IMusbOQomW2wLiLLJmSc9+dWGhY0DXOuxOVtMy4MBTnTkRHAbwSwGMADjPGFsK3FgEczvnImwF8ZhPjK7ELYKVomS3g3MuA6q6F5fqoG15My0gBvJKWGQsKS/4S0RSAewC8nTHWJIrbqjLGGBGxzP7fDW7cv7PH8d4C4C0AcPPNNw8/8hI7BludChlx7iUts+sg+qZWQs9dIlY2yB4TCnnuRKSCG/a7GWP3hpuXiOhI+P4RAMuJ/b8ZwAcAvJ4xtpZ3TMbY+xhjdzLG7jx48OBmzqHENkMEVGXJR9veAuNulp77boUoYNL1kpYZN4pkyxCADwI4zRh7T+KtTwB4U/j6TQA+Hu5/M4B7AbyRMXZ2tMMtsRNhh0ZW1+wtoWVqndK471YsNDgdU9H4XyK/zJYZE4rQMt8B4I0AniKiE+G2XwPwLgD/QERvBnAZwI+F7/0mgAMA/jykbjzG2J0jHXWJHQVBy6iqVWbLlOiLJeG5lwHVsWOgcWeMPQSAerz92pz9fx7Az29yXCV2EXiFKoOmOGhZzli/izGGhsknEMMpRcp2G5abNhTZhyLz36407uNDWaG6y7HYsHB2qbWtY7A9H4ocQFE8NMcs5mW5QbSML4377sNyy0ZFcyDyMYiCUltmTCiN+y7H73/6NH7x7ie2dQyWG0CWAiiyO/aAqgimAiXnvhux0rKhqmb0/1J+YHwojfsux3Orbax37G0dg+X63LgrLtq2D8bGFyATaZCqYpfGfRdisWlAU+Pm2BIFZSrkmFAa912O+bqRUmXcDlheAEniPKofYKzjEca9olvbft4lhsdK24mCqQBApSrk2FAa910My/VR63iw3GCs3nKRcQjjDoy3YYeoTq1oJuxtPu8Sw8FyfbQtH5oarzQlCso2e2NCadx3MebrnLsMWFqZcathC89d4UZ9nFWqQu63opsIGMol/S7CSosb9ZTnLgVwg/I3HAdK476LcbUeB6a2M+fbcn0QebHnPk7jLmiZ0EBYTmkYdgtW2qFxz3ruZRHTWFAa912M+Vps3I1tDC6ajgdZ8qHK3PCOUxmybjqQpQCqWlap7jbEnnts3EvOfXwojfsuxnzKc9++nG/OufM8d2C8ypANw4WmepAl/l2lcd89WM6hZUpVyPGhNO67GGnjvn3eTxxQ7ebc//Dzz+KXPnai10eHRt1woSoOZJkb9VKCYPeAe+4sWnUBJS0zTpTGfRvQsly84c8ewmPP5QpmFganZba/WtP2fMiSH3vuiUKmL59dxvEr6yP7rprhQJZtyFJo3N2ySnW3YKVlo6J5kBLq4ETccy+znkaP0rhvA/7l5AJOzDbw6acWBu/cB7O1DqoV3q5sOzl3nuceQJa7aZnZdWOklFHNsLnnLox7GVDdNVhpWdAS1akAb7PHAPhBadxHjdK4bwP+8fgsAODY5Y17tH7AsNS0MVXlujLbSU/YbgBZ8iERgyL7ES3TsT3UDA/mCIuN6oYDVXEgCVqmz6R2YaUdteQrsf1YalpQE9WpAPfcAcArjfvIURr3LcaVNQPHLtWhKjZOL7T60ilPzzdSvHoSyy0LfgBMTnDjbhQ07vN1E+/4hxOp7kmbAWMMtscghZ60qvhRtsxsja8qRllJ2jA9qIqboGXyz6NhuPihP30Qf3r/uZF9dz9cXuvgf3/X/ZhdN7bk+3YjlltWKpgKcM4dKOsVxoHSuG8x7jk+B4Dh1m86g4ABJ+caPff9hY88jj+6L7/fiUiDnKq2ARTPlvnq+VXcc3we55fbww28B+IuTPyvIrtRhersOh+j6zN4I3h4LdeH7fGAnDDuvVrt3fv1OZhOgOdWO5v+3iI4s9jC1bqFr13svRpz/QDf9e4vbpqO240IAoa1tpPKcQd4EROAb9gq1Q999SL+5eR47ofSuG8hgoDhH4/P4sDeVRw+cBUAcPxKLXdf0/Gx0LCx2s4XBRMevfDci6YECq96vTMa3XVh3IXnLstORMskvVhrBA+voFhUxY2zZXLOmzGGjz56KRzD1hh3IXV8rs+k2bI8zNZMPD3fe0LfLL5+pYb/52+P7zgOu2G68ALkeO58nN+otMyHH76Ezz+zOJZjF2mzdxMRfYmIniGiU0T0tnD7fiK6j4jOhX/3hdtvJ6JHiMgmol8ey6hDnF5o4gf/5EE8sQnueitx7HIN8zUL1x+chaa6mKp2cPxyvnGfCykNoaWShTDuwnMvSssISd6RGffQuMqRcXfRDAOqVxLGfRTZPKIDUyqgmmPcj12u4cIKVx+cr5tbkonRiIx7b219cQ1qPX7TUeDBc6v41MkFLLeswTtn4HgBfvx9j+DxS6N/npZzCpiABC3zDeq5m46PqiqP5dhFPHcPwDsYY3cAeDWAtxLRHQDeCeB+xtitAO4P/w8A6wD+M4A/GMN4UyACTl1tYqm5vZK3RXHPE3NQZR+HDvBl2Mz0Gp64vJ5rfIRhTOqXJzFfM6GrLhTFgywFhQOqIpNlbUTGXfDpkixoGQ/NcMyCcwdGIxMQy/260Uoh77w/+uhlqIqHm49chOEEWxJUFe0Fzy01e+4jxjqqiTUP4lzX2sN/x2LDwqPPrePBsyujHlZUnar1CKh+o1apmq6PynYZd8bYAmPsePi6BeA0gBsAvB7AXeFudwF4Q7jPMmPscQBjf6JmKioAjL37zyhguT4+eXIeBw/MQwkphb3TNdQMD5fXuoNwgtLoZZjmayYqOvfeFTnYgOc+mgnR8tKee7Jhx5W1DkQe/igqSWPjzjv5KFLQFRhe7zj49FMLOHLwCqYm+KpmrpYflB4lxD04V7N6BquNERr3K2sGHr6w2rVd3C8rPei8fhArivn68F7/IKy0Re/UjOcuOPctLmR66Nwq7j0+t6XfmQfL9VHVts9zj0BERwG8EsBjAA4zxkQkYBHA4SGP9RYiOkZEx1ZWNuYp7Kly474b0t2OX67BcAI8L+TaAWDvNF/+5vHuV8JgZMv0cj37uXoHusb5ZFn2Cxv3VsS5j+aaCUMmUWjcFQ9tizfsmKuZmKjwMY6ClmmEKwJV5WOXZb9r0rjniTm4PsONhy+jqvMJckuMe7giChjw3Eo+zy9+o7URTKx/+sVz+E9/e7x7HOGzsNoa/jvWQ+N+tT76jJ/lphANy+fct9pz/+MvPIt3feb0ln5nFn7A4PpsW2kZAAARTQG4B8DbGWOptSfj1meoqZcx9j7G2J2MsTsPHjw4zEcjTGgyJIofrJ2MJ8OsmD3TsSGfmmhBlf0exp0/YF7Q7fUyxjBft1AJjZcseYUrNUftuUcBVTnOljHdACstG6YbYGrIgG8/JD13AJAzKxbGGO5+7BL2zaxjerIVXZ9e6aSjRNP0IqqoF+8ufqPaCDz3q3UTNcNFkAlECkdndQO0jIjvzNVGb9xXWrwxtgiEC2wHLeMHDKeuNrHadrY18Cwco2017kSkghv2uxlj94abl4joSPj+EQDLYxlh/3FhSpfQNHd+CfqTc3VMVkxoajwREQEz0+u5AeHLa3HWRXZl0jBdmE6AakjLSJI3BOfOr9XoOPcMLRNKEDyzwOf/qYlmaj+BIGCwveEMft10IVEQB28lLzVpGI6PS2smrtvHsw9UxYUq+2MxVlk0LQczkw0QWM80UzERNUyvyygPi6sNAwFLSz0AQN3gk/baBmgZsZpbbNqbHl8WK+10Y2yB7aBlnltpw3QDBAw9s9G2AuLerWwXLUNEBOCDAE4zxt6TeOsTAN4Uvn4TgI+PfniDMaHvDlrmxJV1TE91a8nsmVrHs4vtVGNpQWlUNG68hccqIDzRSmTc3SFoGW7U19qj4VWjgKooYgrFw04vcO91elJU0KY9s7seuYTX/uGXh/quuuFAV73IQEiSn8pzF/eBFnr2RPwazW8BLdMwHGiqjckJE+eW+hv3gG1utckYw2KDG6VsvKlhCc99eKMlPHfXZ1gdcV/e5Wa6MbbAdnjuTyVSURcbo48vFIVwyCrKeDLSixz1OwC8EcD3ENGJ8N8PAHgXgNcR0TkA3xv+H0T0PCKaA/D/Avh1IpojopmxjB5AVdv5tMxKy8Zi08GeqXrXe3tn1nkx02z83lrHgekGmAn3z05ewlhVI1rGR6cgpy2yZUaVsZH13IW+zKmr/AGarIoK2vT4Lq12MFczh6qUrRsuFCUeN2U8dzEJio5QAKDrHczWxp/r3rBcKIqLiUoDZ5fzM2aSq6vNrJxadizpkJ34xSp2MwFVALjaJ6j6+VOLODHbfS/3w1LL7MqUAbi2DNDfuI86lTVp3Be20bhHtMx2ee6MsYcYY8QY+2bG2CvCf59mjK0xxl7LGLuVMfa9jLH1cP9FxtiNjLEZxtje8HXv/LBNoqKxHe+5n5zjD0KucZ/m255I5LsLvn16kt+EXcY947nzgGpRzp3fUE3THwnfGAVUw+W1GhrWZxaa0FU36peZNeKd0NBljVM/cOMeGy1Z8lLnHRc5xUaqqhtblC3jQZVdTE20cHnNyM3bTq6uNsO7LyUMUjJV1vMDGOEKaWUDee61jgsROuu32vm9f3kG7//Kc0Mde7lldxUwAUnPPf9ePL/cwrf9/hdGWsV5cq4eBfqXmtvoue8Ezn0nY0KLecadiifnOBc7PdVdmagqLmYmWykRMZEGuaeP5y5LAbRQF1uWihl3xhg6NtddZxhNMU0sPyBSIfk4Lq52UK20e1aSdkIaqlcefx5qhp3y3GXJT8kuRNk0Cc+9qptoWf5YG4gIo6ooLiarbfgB15rJIjnWzayckt5mcnKMdfTZhmiZmuFE6aNX+wShDdtL0YiDkNcYW0AaQMv8xQMXsNJy8F/ueXIkuj08mNrAdfuWIFGwrZ67WMmVxr0HJvS4gGSn4uRsHVOT7Si/PYs902s4dnk98qTFTTwTTgZZXvVqw8RExYy4Z1n2ColzWWEQqRp6LaPI2og99zjPHQAYAyp6JzL62ZjARjz3muGkDLcs+Smp48hzTwSthSTyODNmhKFTFTfKDsqTIUh57puYWBebSc89PtdGonl43fCGXpmttS1U9Q5Uxe97vSzPH8q4i4kmm+MO9DfuCw0T/3xiHocOXIXt23j7x74+tEZRw3AjRwLgwVTLZZiZqqOiOzvCc9+2gOpOx4RGfbNlGGP4yfc/OjZxnkFgjOHEXA0zk/kyAwCwd3oNHTvAs4vcMFxZN1DVnNDT6aadFhoWVDX2YrgHO5i7FoJeE6HBG0XGTJdwmBL/FtWKASL+XtZzb4eedC95hTw0Q0VIAUn2U3RPlCopx8cU6ZBz6+Mz7uL+UxQPk9U2AJYbVDVdP5rgN1NnkAwCNhLXT8SeJqttBGz4CaRmONBUB1Xd7Ou5W26Atl18/JH0QA7nThHn3j0R/fVDFxEwhtuPnsLtz38ST1yu48++dKHw9wLAz3/kcbzlb45F/xdCfXum6tA1AwuN8VN2vbAjUiF3MiZ0fmP0CsyZro+vXlgbi15GEczVuBc1k8O3C+yb4WMT1MyVdQOVShtEgK76XcZ9pWWm1PVkmaslDkpfE6Jhgm8cRVCVX3cWcafCcweAidCwKnLQpd7YsYVxL2YkbM+H6QZRJgwQTmqJFUvDdCERS+VSi3TRcXruwqiqMhc0m6xYubnupuNzyQgp2LTnXtFcKLKfun7iPpkM9YaGkSBgjKFmeFBVB5rWwXyPQibXD+AH8e9XBHmNsQV6ee4N08VHH7uM5x2YR7Vi4vpD8zhycA7vvf/sUMJrc7UOvnp+LfrMU/MNKLKPyWobumqOpWCrKErOfQCqGv/bK2OmFt7825VR86QIpk739twruomqbkdysZfXOqjo3ACripsy7owxrLYdaFoysDi4cQUQ0weCqhiVcZcllkhPDKKqQ/E9eRW0IrunXjAY3sgUMAH8vO2Eca+bvHl2MpdaU23IUlAo171puRtKjRO0mYgHTFQbOLfUbdwNx4ckedA0d1PXfrFhQddMaKqXS8sI4z4M7266PlyfQVUcVDSz52QonKhOwdRboLeuDNA7FfLuxy7DdAIcveF8tO325z+FgHFxtKIQcYj3P8gDwE/N1zE92eCOk25huWVvW4s/QaXuCPmBnYgJnT/JvfRlBK+8XfozJ+cakKQA0xO9E4aIgD3TK3js4iocL8BS047THGU75Z11HB+WyzKeO7+BB+W6t8bgudteAEWOH0wiQFXSkwj3sLOe+3Ccez2HT5dlP6UV3zDdFG0jxlOtWIU893d95gx+9C8fHvphjzz38LwnJ9p4brXTxQ8brg9Z9qAq9qbiHVfrBjTVgJK5NwQ9tBHjLu4FQcvUjfzCOPE7GnZx474cNsbWtO5zzitisj0fH3zoORzYu4KZqfi50VQXFc3FlYIyzkHA0LEDSJKPT528irmagVNXG5gOKdKKZsJyWeEiyFFPAlGee+m552Mi9NwbPX4gcdMW9RBHjSdna5iZbEb5vL2wb2YdKy0XX7vI894FL64oTiqjRGiGaIm0MrmPQmISwririgNN8UbmuYtgqoCieCCwqAhLkrwu2syIAqqDx8AYw8ce560JdS020tkVSz1snp2FrrVT8sO9sNSwMFvr7bX2Qsy583tsqtqC6zPMZtIJDceDJLlQFGtT+jKLTQsV3YKi2Knrl/XcV4bQl0lKO4gU26s5fLRYKdkeKxywXWlZXY2xBShHW+b45TrW2i5uft7Frv0rehtXcoT28tCyPTAANx6+jCBg+K2Pn4LlMuwJExVEauZigaDqw+dX8dLf+tyGKn97oaRlBkAY956ee3jzb0e6pB8wnJyPPYV+ELy7UKoTGS2q7KYeYFGckue5F6VlFNmDpjqpgCpjDF96dnnoDAvLjeUABBTZxUTFjiY0SXJTE4/nB7A9/l4Rz/3PvnQeH3zoIm4+8hz2JugtKce4J1MlBaq6gfkCtIwo5X+ih8Z+L0RZOnLauD63kg6qdmzeHlBVnA0bd8v1UTc86JoJVXFRS9zXDdOFJAXQNQsSBUMFzJOeuwhC5wVVk/dY0cK55aadmpSTiDj3RF2AyG7R9e7PVPUOLq4V6yImbML0ZAOHDlzF/We4QoqIf1V0bljRoqEAACAASURBVNSLBFWfvtpAx/FH1sEM4L8lgUGVafDOG8DuN+6ClunFuYc37XYUOj230obpBNgzPbiab2qiCU3x8NlTXBcl9tzd1MQVe+7dnPugXHeRoaIoHhTFTomHHb9Sx89+6HE88OxwEkF5nntFb2NqMg5gS5kK2mT64qA89488cgl/8PmzOHJwFrc//+kUny4Cp0Irvp5JlRSo6iZv1D1oZROO5dil4Yx703JBYNEkq6r595zheJBlH5rqRLGgYSHUFSuaBVVxUivSpuVCU3zOJ2vuUMqQyUYoIgida9wT17AoNbPYNHOlB4CE555wKsT9kXUaAO70LDbsQnIFSbrs6A2ccxfBVADRyrJIOqToGTHKwLzp+FAUBsoK7owIu9+4D/Dc10VAdRvExUSBhOC4+4GLiK2FQbcgWjKqioumFcv+5nvuxWiZpOeuqlaKkz2z2EyNuSgsLwBlHsJvvu04XnZbLEebLTZKGoVaHw/25Fwdv/nxUzi4fxEvveVEl+iULKVXLA3LSwVcBWJ1yP7eu0gVffxStwZQPzRNF6rqR+MTGUOtTP2F6fiQJT7GtuVvSE9FeJkVnXvuzYQkdDLmoKm9WzTmQaygNNUJ7z2Wq+uepNeK5rovNa3cNEiA3/eSFKSuhcisyipIAtzpCVj/IiuBiC6TXeydrmH/njXsnVmLfifxjBW558UEMEqdItP1oUjj09TZ9ca9qvO/vTxzQWnY3vAqhJtFbEyLeWn7ZrhRmahY0Q2oKi78IOaoV1o2CCxV7derUCiLls27NkkSTylMeu4iL3sYnhbgbfYkSj/kkhREee9ifEaOUZAkr29K4JOhfskdLziZG7NIcu5+wNC2/HzPvcIfyEEyBMIYn11qD5Vd1bTS+fcisJo1foYbRJ47sLFCJsEP65oFVXHh+iye3Aw3ijmoijlUqz1ByyiKC0liqOrOYFqmgHH3A4b1jpsrPSAgEUvRMuYAzx1AboObLFqR587/vurFj+AVL/pa/L0SQ0VzC3ruoXEfpefu+qlkhFFj1xt3VSbIUtCzSjUZNNxq713klYuS/EHYFzbvqGhxGp3gkGOdbhu65mboiTBbZhDnbnlQFb6PpjqoddzI6xN52cOWrZuuF2U89IIspykRQR9VdbNnIBwAZkOZhV6GIbliiR/kfM4d6G/chTTDzFQNAQO+fiWm0tY7Tl+6qmm6qQlcCifQ7ARhOT5kKS7Dr22gkEmkalY0M7o3hNfdMB0oYQGXpg3rufMguwh6VvROriFLVkIX4dzXOjYClp/jLiBJLOW5Cyclz7gLurJIgFzYBBHoluUAcsaY6pqBhQKdpxbDFdPsCOWjLbdb336U2PXGHQA01YvyoLNIppxtNe8uPDe5oHGfma5DknxMVGMaR3gdUfu0lt2VLxxnywzg3G0vmmg01YEXxGN8NqRlhjXuPM+9/w0qSX7ucr6im7C93gVos+tGSmYh77hiDHmKkAI8wMj6el22F8ALgAN7V0BgeCJR9PYb//w0fu7Dj/eMaTQstytLR1P8aHIHeDaIFyBMheT35EaylRabFlTFh6LEqxRx7nXTic5fU22sJybvQVg33GhFAXCjN5ejpmmlPPfBhinqwNTXcw9SnLvw3LOxHHEcWQqKGXdRf9Bn5aypJhYa/Y/FGIuqbGcLpmEWgel0x6tGiWvCuCuy03MZvd6xoxZw22Xci3rushTgW1/yMF5w47loW9a4LzctqFnjXpRztzzIssgX5zfresdBw3Cx2o4nj2Fg5gRUs5BlP+XxCc5deNS9fpcr6x3oWu/shCQtEytCdh+LCJioGjjVp7IxmnA0CzNTLTweZsw8t9LGp59aQMB6860Nw+maVBTZS3HuSW90U7RMw0IliseINF/+t5Wgh3TVgeuzwrpLtY6TUtys6CYWG91NO9LNUQYfW1BDfT33DC1jhTINeZM6ETBRMQulQzYTCQS9UNHNgamQTcuD7TFIko+FnGuyUfBnZ3xswjVh3GXZ6RlQXTOciKfb6kKmdsRxF78Z9s7UUg+Cmll6L7etVDAViAOLg2iZlu1CksJle3jctY6D8yucklEUB8ut4ThF2/VT/HoeZMmHF8S5zGI5L/Kpexm5K+tGVAiVe9zEpFbvY9wB4OC+q3jo/GrPyStJoe2ZXsWJKzV4foC//PKFqH/kXA/Pv2G5URpkPDYnxbmbiSChMO4b8dyvNkxoYe9cUdDVMLiHntTeEdRP0ZXYeseGmjDuVd3MbdoxbEC1iOdOmYAqDzz3vqcqeguX1wenJPLr4efm1wvomoWG6fdcPQLcoQKAmcn6SBuZGI4HpfTc+0NR3J4pdXXDjWiOrfbcuSe1uZlZPKxNkz/Aa203VcAEiCo/ViDVz03RMgD32EQwdf/MGtbaw10j2wsGe+6ZfPRO5Lnnd5oCuNfVsvzIu+93XCPlueffB9cfmkXAgE88eTX3/VbCuO+dXofpBvjC6WXcc3wOh/Zz0blennvL8rq8Q0lyU6tJ4eWKbBlgY6qciw0zkUkVx2M6jo+AxbSUSJUtqi+zbtipiTEqZMrw0SnPvQAts9SjMXYSWVrGcPpz0dWKgctrxkDKqWm5A58/keveL6gqzmHvDF/NjSpjhnPu2xhQJaKbiOhLRPQMEZ0ioreF2/cT0X1EdC78uy/cTkT0J0R0nohOEtGrxjb6EFx/pfsmNh0fjseiIMxW68skOe6NQknQMk3Lg+uzLs+diItzDc6WcSMjJDy7tY6Ds0ttyFKAPdM1mG5QKAtCwPa6i5iyiPPR0/n4IkUxz7gL2eO+nrsUH7cR5Wnn/8ZTE23smWrgnidmc98XaZCK4kYFZf/1n54CYwy3P//pnpy96wcwnaDre1XFjfLmgQQtI/uQJAZV8bE+JC3jBwwrLSfKz444dzOuhRAriGE997rhRvn5QNK4p885Sa8V8txbFnTV7bt6JQq6aJl+99REpQPDCQbWCvBAd/9rLK5lv3RIYfj3hgkPo8qYMZ3B8arNoIjn7gF4B2PsDgCvBvBWIroDwDsB3M8YuxXA/eH/AeD7Adwa/nsLgL8Y+agzUGQ3l1sUD4/IM+8VdB0X2pYbcdwbhSLzUv6G6cYCTDn8ZRHj3g4bdQBxoc16x8G55RamJlrRpFHUIDDGYLussOcuxhcHVPlDkzcxFzHuUkHOXeDIwSt4ZqEVSSsnkaRlKrqFiYqFtY6DIwdnUa2YqOpWrscWe/zdnHvSmYjS++RkttJwxn21zTNPBOcuS7wmom7E4nJKxLkX/y0tl+sVaakOVqGaZi1r3DllouR0/3r4/Co+FxbhCfTqwJQEUYaWGcBFi3siryFKEk3LhTzAuIux9fXcW8K4c899VJ29zO323BljC4yx4+HrFoDTAG4A8HoAd4W73QXgDeHr1wP4CON4FMBeIjoy8pEnoCou2pbfFegQD09Fs6DI3dK540bL9gbeXINABGih7G8knZrT0YanG/bXtW9b8UpClvhDut5xcHapiYlqM6J7ihp3xw/AgMGce6Ybk+Hw/N4oJTDXc+cP0ERf484gEdeKrxtcArdfWuaRg/OQiOHer891vZfNbNoztQqA4fk38OC2rndylSWbPSYVRXGjlobinIF4olMVe2jOXXiXejgpEgG64qFhOl2TG5+8uYLoIETVqQnPXZG5fHI2HmKGv50i+6nzA4A/e+A8fvdTp1Lblhq9q1MFuHFP0jJeX+NeNB2Sp4b2f+aLeO7LTRua4kHX+N9R0jI7poiJiI4CeCWAxwAcZoyJDhiLAA6Hr28AkFz7zoXbssd6CxEdI6JjKysrQw47DVVxEbDuvFvx8KiqA1Xxtt64m86maRkglv3t19EmT3kxCZHqJzw7UaJ+Zc3AUtNJee4rrd4G4Z33nMSD5/jvJZbogzx3KVNJyukqP+V5ZjFbM6ApXl9PHOArFtPhk582gF/VVAfX7VvCvcdnuzR0sgVnL7z5Wbzi9mOYnODeYUU3chttxxkZ3Z57x/YjXjgKqEpCvG148bBkjnv0PYqDuuF2TTISMVQ0r9BELfLtk/EKTvV1SzULyiTPc68ZDq7WrFRwcqllFfDc/Uyeu9f3nhJxmEFt9/JUQrPgKaV+X6nnpaYVPXOVgjpFRWCFRW3jQmHjTkRTAO4B8PZsw2vG7+Ch8oMYY+9jjN3JGLvz4MGDw3y0C0omXVAgqZeR1UXfCrRtr3COez/Iip2mZXK7yLt9aZm8tExVsaImJlPV1kCe1vZ8/P3js/j0UwvR//l3D86WARKce3hdkp5nFrPrBip9vPbo2HLA89xNN1c0LIsjB2ex0nLx8IW0JnismClkczs4fCDu3lXVTay23K7G16IwLpsto8geAhZPaKYrVgah564OLx62GEkPxL8/V4bspmX4udiF9GVEFXcyz50fq7udnun6kGQfsux2xWbqhgMG4EIomBYEfOWQt9JMQiIGJ5st0+e5kWUfVd0ZWKXatLzcuocsKprV17gvNuPOZ7reGUkhU1T3sN2eOxGp4Ib9bsbYveHmJUG3hH9FCd88gJsSH78x3DY2CG8rW4FaSyjdybJd2Lj/3deu4L1fOLvpcY0ioArwPP666WClbUMiluuNSJLX37jnVMsqih0pB05OtKOHu1e6oPCwxXJYyL8WDaiK8XVCjRU+BifXc7+83kFFG1wwIlYsDTNf7jeLg/uXoCkePn4inTXTsryosjQPVd0AQ7eCYE/PXUnryxgZz13rcd79sNC0IElBysNWFAfrhp2bLaSqZqRF1A/rCScoCVnyurxzyw0gEZcuzhp+MQahnFgzHPhB/zRIgKdCOl6SwvIG3lMVvdWXlgkCho7lD6RlAEBTDSw0e1Mtiw0zinNUdS4JvVltd7G66dVXeRQoki1DAD4I4DRj7D2Jtz4B4E3h6zcB+Hhi+0+HWTOvBtBI0DdjQZQumMmG4aJhvLtMVhe9H+55Yg53P3Z5U2MS5eyjomXqBvfCdM3JLe6QJR9Gn9ZneZ67MOayFGCi0oEkMeiq29NzF2l1V8JAVrY5di90p0LGxVTC80yCMYb5mtk3mCogSVzaoNbJV4TsHkuAqckaLq6m86Tbtgu1z4NW6RFg7Mm5h9dZyCIk89wB7rlb7uD01SSu1i1UNTv1+/N7wwkTCljm97WxWkBfppYQDUtClt0uXt0Kg52y7KWMu+sH6Nh8YhTGfalAjjvAUyG9TEB1kHGv6gYu9ZH+bTtcy73IPaHrZk/ZX9H5TE8Yd8MJBkqZMMbwV1++0DOzJg6wb6/n/h0A3gjge4joRPjvBwC8C8DriOgcgO8N/w8AnwbwHIDzAN4P4BdHP+w0slWcAnXDgR6q9aly/vI/D3P1DlZaTtcSfBhkOe7NgFNKHlba3dIDArLs99X6EB5kMntHC4tWJqvthFJeb00SQXMtNCz4AYs494Gee8a4t2034uFVxcF6J31OK20btsei4rN+kCQv8tx75bhnoeVQIu2cXPUkIn2aekHPXe7huYtsGZHrXjAdMggYHj6/gumpdC9goQzZNGO5XwFdtbFaIGgrVrjZ6ydJblevVMGHK5KXapKdLBAUxr1IdSrAjXuSlinCRVcrHSw3nZ7FR80cmqoXdNXGetvJ9cbrBhdnE8Y9arg+QGF0pWXjf3zmDO59ojt4D8Qy1eNMhVQG7cAYewhAL8Hh1+bszwC8dZPjGgriB8xWoK53nCgDQFEcrDYGe9GeH2C56URL8G86MLmhMQ0rPdAPiuKgaXq50gMCsuTB7FNUEo0nYcDEtZmoxiEURTF70jIiQO0FPMhUmHPPyCMk6SpVcVHL/G5RpkyfAiYB4bk3TA9HpotNpKrioNZIGzI+pt6f55476/LcG6YLItbdsEQRnnts3JP9ZZOpqNfvrQ4c88n5BtY6Ll52fTrVUFUcmG6AlbadakEI8JRZ0+EB5359OmuGwys5M7noiuxFPQAEREBVlj0YCe811pVnOLvE76flVjHPnaRYfoAxltsAJouJCqfJ5usmXnhwquv9KBZSoIhQ02x4Af/Mngk19d5SNEEJzz0WoXvJ9Xt6HlNQXb34eTOiZXZItsxORS/PvdZxoMixLrrhBF19LbNYbvFcYmBz+azDKkL2gyrzbKDL60bPSj9Z9mG6vc9NeFl5tMzURCuxze4pQZD0Mudq5oY9904i0Kwq3dIRIuWwCC0jSz7qpgPby49F5EFTHTRNP5Ux07S8SJohD0IGN7vMbpoeNMXrosrERCEmVdPxUg/ysPoyX3hmCUQM1+1Lq1OKc55dN7omJ5FV87o/egDvue8sLq3mr4RqHaeLkgHyV4OG60XGPdkkW1Br05MNXF4zQicpbRh7QUqkQjp+gIANvqfEqq6Xxky0oirEuYdZYjkr1iy1JOSjB6VDigykXhk9YsWx3UVMOx5RQDVTyMT1MtKVi4OElJIPb15ec1GM0nMXY+/Yfm4BE8BvEmvIgKq4qZPGXdfsnhIEybzs2XWjMOceFRtFFap+ynO33LQypHhg+0kPCMhSnMZWlAJTVRsM6f6tLWtw2qqudbrS4HqVuMeeOx+T4fgpHZFhlSHvO72IfTPr0DLeuTjO5TWjK6D8vINX8dJbj8Nks/iT+8/itX/4QK6BrxluSjQsOocwnTMJK8yWUWQPphMkGoXwceybWYcXcEdkucXzwgdlhCSLmARdIQ2gZUSu+1995QLuP73U1fikVywkD/0KvkRxU1KsTZaCgVWqYtK+3ENFcqdw7jseRICmeF0e4JoReyS9vPssrqaM+8Y99zyOe6NIGq1eaWWy7KXEubrGk9M4ZN/MGm44dAX798RpgZpqw3SDXMU/vhLiN+VczYTlFbtBRc606fK8byOR6pbXkm62ZqCqOYVufClRTFOYc8/hu1uW25dzB/Jz3ZtmfhVkF+fuptP7Yk33wWOeqxl4drGNg/sWu95L3tdZQyYRww2H5nDnSx7BK1/8Nfgsvxn0WseGmpNpJOdkYAnKRJa5lo1YvQnPXZTon1tqh71TBwd0eSemsB4g8mgH1SzYeP4N5/D12SW8+a5j+Nb/fh++eGYpej+r5T7oWEC+Do9YfQinSqhSDvTcw3trsWHnsgX9GpKMCteEcQc4t5Y17nXDjR74Xrx8FmJG1lV3U5VonRyOe6NIPrS9glODujG1Ld6IIcmPa6qLl956IuV5Rl5MTiHTuuGiotmo6jZma0bhIiY+Ps79On4AP0DkxWZVLwG+KtD1YrrZcsobLk7LAMB6ollGUpqhF6q6iYWGnaJzGj2Ne5pz59rdyRoD3nc1z3N/4NnlqP4AAO4/zamYg/vzjHu6qrQXhAHLn7TtVHWqgCxzqVsvFezksQNxfoK2iYx7KK51YaXdt3dqEkQMXmjcY4G1/vcUEXDb0dP4V3d+Fq988WMwHAufeSq+PkW03AX61XcsNW3oanr1oWmd3IK2JMSkHbD86lexyi459wKQM5ruQjRM3YDnrqsuJqrNTRUrjJaWiR88rafn3l/TvW1zhcpBvXiFh7LS7r4h1zs2FMWCrvFS/CigSgWMe+i5i2W+nKBlgDRFcnm9g2pR4y53Ux2DoOZI7hZJW63qBvwAqfZ1PEun+57iGVpxEVC2pJ4IqFbs3FztX733JH76g49FfW2/cHoJU9UOJqvd1yQZRO03uUXGOCfoXks4QXmfEVLSQktIluKYiXBieECVoaKZmNAtnF9uY6lpFvPcKYAXZDz3gvnfksRwaP8SJqqt1PNaRMtdQFMdEBjWetAy2XOo6sZAWibpOOTx7qXnPgRkOd0Jfj1TdZeUR+2H+ZqJim6iqhu4somuK60xcO5Ab+nU2HPP/75BqX7Z4+dJEKy1baiqHUquduKAagHvgzfJ9uMVTSKgCsTZFp4fYLFhFwqmiuMKFPbcM3y35fpwfTZwCV/JCaY1Lbend6gqXsS5d2yvi2qoVppRe0OBju1hocGpsTff9Thm1w08cmEV1+3LLxVJFS31GX9cSJYeg+MFMJygZ0AViKV9bS/UEpL9rsmikUg7rlabOLvUxGrbHZgGCcS0DM+U2ZjRq+qdlJBYES13AS7F4WElh5bhq4/0vVitGKgbHjo2z2A7t9TqSqOsGw4k4s9FnpO4FZz7wFTI3QJVcSPZV6A7d1c8gIOM+1zNgK51UK0YuLjqwPUDqPLwc+BIs2USD23PgKqc1m/Joml5kKXNLVHXOza0qgNds3Fp1Y4MdRFaRuSjx/negnNPe+4LDQsBK5YpA2SMu1rMuKuZTJWiqywR4J2vm7gz3NayPDyvRwpmstDHcLwub3Sq2sKFlTaCgEGS+JLquRVuoI7ecB5XFl6Af/+XD8ML8ikZMWYCAwP1nZzExJL13EVhX97EIK6HOAc7ouHiPPSIlklw/pPVFs4stgpVpwI8oAoAXsBgivzvISs3qxUDl9bs6HktouWehKZavT33SvocREHbK37381Gs4GNveTW+/QUHon3WDQdTE220jOkotTcJM6JlSs99IFTFRSNBy9S6PPeCtEyDe+4V3UDA0FdzQmC1beOr59NaJW2b5z+Pokci12FhodRq/g0rS4NomWLyw70kCBhjqBkeVNVBtcKvDc+8YJGH0g8iHz2rvpjl3COp3wKZMgAgRdeDFeJXAc7/q7Ifee5FJ2LxUItAu+MFsNzeHr8sO4k89+6S+smJNiyXYSER5BRdsW44dAV3vOBJLIWKhILLzoKrhqYprjwoGRpFQMhg59EyUeN1J+04cOGwtOGvJzJupibaEDR9vyYdAsK7dv2gMOeehbgnRbPrIlruSSiK1ZUKGUT6+elzuG7vCq4/dAXXHz6PW24+AwBdOjfrHRuqamEijE9lEa9QSs99IBTFxXqiqGI947nLcgBZCvoGVJuWi44d4EbdjPSsZ2sGbto/kbu/HzD87WOX8e7PnkHb9vHVd34PbggLUtoWXxYO4riLQGQD9eorCXTrt2TRjz5IopcEgeFw6kJTnMjwnltuQ5ZYoXOUJB+GG2uVCI9FlnxIFES0zFNhn9OJ6uA2auLzAJdFHuZaa6obre6Keu6K7PNAe8i3Cl63l1GV5bgbk+kGmK5mjHt4jheW29F9c365DSKGiUoHUxNt2K4eXqPe9IKquLBdta9wmiTxYHonc3+0+gT+swY8SZlErR3DlUCtY0MJM26mqsnU2gKee2jgXJ8VzpbJIi4uMnDzgYlCWu5J6KqNlUx9x1rHQcC6z0FTHbzs1hMAAD+QcP7K7ak4DBCuclUHjLV7cu48wWE0/VjzcO0Yd9mF6QbRsiwpGiagKV7fbkyCS63oce/ObDqkHzCcW27h61fq+Oijl3DqaitsBjKJxYYZPaQt24M6AkpGQFX6qx5GD1sP496y3MIKlXkSBOuJ6ymuzYWVdmHOUJY9GLYXeY5iLESArnqoGy78gOFvHr2EfTPrqOqDjQKQ1EcfLuVUUa0oLjNM2iqXfA2N+4CMDEV20QxpD9PxsTdjsCZDI3hhpY1/dRtXRj2/3MZk1Yge+hfceH7wuSg2gImBNIQqd6e4ZmMgSWQNuJmoa8iuBHiVa0jLTMQTcyHOnYRxD+LaiQ3QMkDMbxfRck9CU20sN9P7LxUowpKlAJrida10a4aLA/sdSOTnxu5MJxgr3w5cQ8Zd3Fgty8P+SS0UQ0ovmZUBsr8ix72im6hoJggsZdzve2YJb//Y8Uggqapb+ObbnsZktY1HnnxN6gduW14hY1EUU5P1VAPjLOKGGD0CqraHyalixj1PgiDZ0EFcG8PxUdULGnfJh+nGRTFKwtBxZUgHDzy7jLmahZe/6LlCxxTHBTDUEhwI5XDDCawVeeAFAs56nAbXjGSCexh3xUOr4yEIGGyPdfHImupAU71IiwUAb5xSaWYP1Rdi0lcHXAM5pyipn3HPpjsmM1nkzHt108W+fbEDUNFcWI46FOfOaZmNBVQrugkiFvHbDdMtHIMB8qUallvFKmx1zY6kFsR5dOwAR1QHUBnml11Yro+KGktAmK4/1ubYwDVo3BumGxp3Hr1PLmdl2e7bak8Y96pu8nLzipOqUv3AgxfAqI2X3XoGe6ZrmKh0QATYjg4gzVMnlQ9HgZe/6Fjf92POPd/Ydmwfe/YWM+5cgiC7zAwfXMUJr40Nw6oUfgiFNG/EqWakh+uGiw8/fBFV3Y4aUhc6bmgw8yos+0FTHKyH4mHZOEA/VHUTV1dMnFtq4dkwVbFXFpIie1i3/Z5pb7wgphXpnztegMtrJo7e0N0GsB/EvT8o2ydPwjdeteR47iJoGk4IVg7n3rE9+AFD2/JxMHG/VytN+P7eQgFDsUpxPbbhFEGJWIrfbloe9leH89wBHj8TNGxRVUtVNVJt+pJ9JMTqYa5m4pZDsQaOHVb6jhPXjHHPFimtd7pVAlXFQS2hDHlhpY0pXcHhmQoAYL7O9bLFD61r7WgJ3rJcHLtcw81H5nH9obTSm6baILDU7N203ELZKaNCNviVhOMFcLziAUddtbGcEdZKdrUC+LUxrErhgLEs+zCdIKomTT70iuLg9GITdcPFLTc/NxQPuVFaRlUdLDfS+i9Frs9EpQPLZXjdH30l2tar9kBQhf0mj8lqnA55ea2DgMVcfOFzUXpnvCSRlekFCnruGc5dkvyoxWHH8dGyXC6vm/CUD+1fgK4VKwKMaJkgCIt70sV2RaHrbVxZNyIt90NDOFd6jnGfXTcgERtILemqjaWEHnw9IaEsrsFszUgZ9yKyxpvFNWPcs9kwNcOBoqRnXEXxonRJxhh+8gOP4oUHp3D3z78aAE9xm9BjveyKbuDKOn/Qvnp+DX6ALuEmIG5Zl/TcW7Y7kjTIouiXLdPvAc6DpsUSBBMav0XWMzGMasVArQlQwcCXLPmw3AAdm6fuJScFTXGxYriQpAA3Hh5OR194P8Mad01xYDqc442aXBegZW44dAW6ZiEIpOh784qLkscT90XewzxZNW+ppgAAHWRJREFUbWN+mdOFgp6ZnBjOc9+/ZxWmXR04KXIJ3/Q59gsmZ6mXrFCcqvDfs56TcXP0huLUWpaWUeRgQ4kIVb2DK+sddIbQchcQ93VSguDiaofHPwbkyuuahat1LhlMRKlkDqGBM5cJqpquX/jZ2SiuGeOeXP4AwHrbzvXc11v8gp5damOxYWO5aaPWcbBvUsN8zYCmxV5TVTdwadWB5wf48tllqLIfaWdkoWlWF+euT26dcReelJGT554n99sPSQmCmw/wW6RmOCCKvX+RnVCkOhXgBiFgPK9aUdIPr/idDh+Yh64Nx53HhmZIzl2NUzDbtgeJgkIpnbIc4PCB/Jzzru8Ir5WguHI99zD4eGGlHRv3IT33wwcWC42JC4GlDV7b9nt2oBKaQGJCMDN8uBAWqw8h0pWHyHMPaZmNluRXKwbml91oBT1MLwVRP5JMJLiw0kJFHzzR6poNx2No2R5mKmpcY6M60FQbshRgNpOYYTo+JBqvfbhm8twnqx1MTXTw2598Gp87tYg1w+nSy1BkFx3LRxAwfOUsb/IcMF7eDQDzdSPKZQa4vKfQhvjimSXs27Pc0zvSFDPSfgaKlbOPGqJZdBaRZ1pwmZonQbDecaGrbmSURXZCcVqGj2G15XTxsOJ3uvnIxULHSh1XGJphPXchFtWx0bLckaWtpsYWnrPgbnt57gBPhzy/0sZExRpbYUuehG/H9vp2oFLkOH0yKw3AA7ReVIA27AQrEKVCBsGm6ArhcDxztRmOZ5giJnE/hJowAcOlNaPQRCsa6CyHv3PU2UpxIqGxbDpkXlHbqFGkzd5fE9EyET2d2PZyInqEiJ4iok8S0Uy4XSOiD4XbnySi14xx7ClIUoBvfemDmKjW8B8/+gRWW05UZi6gKpwbbNkevnx2GVMT/GH6zNMLcP0gLFhIGPfwZvnSs8tYajq4bt8SekHTbCyHvJsfMJhuMFQq1igg53SkB4bXuRGee1KCIKv5La5N0QdRTAJrHbtrHDccuoKXv+hx7J2uFzpWaqyahRsPX8LBPr9NHiJlyI5bWJphWAhPVjz0eQ9ztWJAkgKcX2nj3FIL1SEzZYaBLHldee5tu/+5CwMOdLdVlCQXHcdL9G/dqOcuAqpB1MZvIxAOx6nQuA/z/MlSAFXxo9X3QtOC4zFM9KDckhCcvFihJTPLABELSB/HdAf3id0sinjuHwbwfZltHwDwTsbYywD8E4BfCbf/AgCE218H4A+JaMtWB5rq4Fte8lUc2LuEgKHLc48fNguPXVzHgT1LOLh/Hg+eW+Vl4CwW4wfiisSPPsp54IM5fLuArllY67gIAjZS0bBhwDNSupe0eY06+iGuxIy9jXXDhizHnvzQnnu432rbgZQJNOuag+ddt7E2u0TAS245iamJIYOQQjzMcAZ2YdooIs9d0DI5RksihqmqgXNLbR7grw7Htw8DRfainHWBdo7mTRKy5OZmy/C/PECbx7kPgzjPnYUdqzb23ExExr0Rjme431RX4/TYi6EMRBHPXWTTiIlBSGOL6lOuU5Xh3J3xB1QHGl7G2FcAZInm2wCIdIH7APxI+PoOAF8MP7cMoA5EMhxbAkX28Yrbv4YXv+Akrj+YzmoRP/bnn1mC6zMc2LeMwwcW4PoMdz96BQDStEzYWu3sUhvTky1U+hTW6KoNP+D5vsNy3KOCLHswczz3oWkZ1UFFc3B6ITY0a2075blXNAtEQeHyaeG1rrbsLuO+HYg6IXW4REC/LkwbRcS59/HcAZ42+Ohza7A9NnQwdRgICd+kZHHH9iD1uS8kOU6ftNwABAYKPW05bMMXG/eN/a5xhWrQJY08DAS/LWiZYak6VbWigKpooD5RGd64rxsO9ETmUFU30LL8tGqt649EmqQfNupVnwLw+vD1jwK4KXz9JID/g4gUIno+gG9JvJcCEb2FiI4R0bGVlZUNDiMfksRw85FLXcZYFHt88smrkKUA+2fWsXd6HRXNwT+GjWyrmpE4ToCqzj9zYG//ZX9yadYZIrVulKCc5gq25+PBc1z3ZpjJZnKijmcWYpokm1pKBNz0vEs4sLf3aiYJ4aW0Ev1TtxP8wed66i1rPJlN4nrHAdX8h3my2op+t2FXIEONJ0cZcpAshSK5UZGXCHaK2IQie+g4Hupmfg/WopBS2TIb56IFvy1482GfP1W1ohaTz612uNxEgQpbReZ67yKQW890toqq3RMCYkWagG8WGzXuPwfgF4noCQDTAMRT/9cA5gAcA/DHAB4GkHsGjLH3McbuZIzdefDgwQ0OYzgIz+LMYgt7Z9Ygh1otB/dfjYJF2QmhEuqKXzfAuGtqPHu3RqgIOQw4pxp/58m5On7wTx7EPz4xhxsPX+qZj52H6ckmzi934PoBgoChYXpdsrAvfsHTOHLwarGxJW7kojII44REDLrKxcOaljOWVZb4/SPPvYdHmizXnxwjLRPXQsS/Rdt2U9XC3Z/xUxWqyZJ5wcc3DBfaBr12ANFKgNMym+OidT2+lsP+prrqRBPDxRWun18kyE4EVDQn6tq01rFzjXtSQKxIE/DNYkOpkIyxMwD+DQAQ0W0AfjDc7gH4JbEfET0M4OzmhzkaJJdp1yU8zkMHFjC7eBQVze2aTat6By15Bvtm8lMgBcQMv9KycSCsVdhqIyZLPgzbg+MF+JP7z+HPHzgPXbPxqhd/HQf3F/OwBaYnGnB9hudWOjg8o+fGMIYdm8BO8NwBTs1wzt3HtD76VZYUCn4JtcFentpUyOvqqtvVI3WUEJNL2/ZwONw2KGVXkb1INdPKZLLIsgfD4aJv/XSPBkFK0jKuD1nduNETgX5VLqblnoSm2mgYHjw/wIWVFqqV4hOtqhqR555NwxZjEhkzbsEm4JvFhow7ER1ijC2HwdJfB/CX4fYJAMQY6xDR6wB4jLFnRjfczSHJCSbphP0zq9AUD7rWrd52y81ncNORiwOXnELadLllQ1e4hsSWe+6yj5W2jX/35w/h1NUWrj90Bbc//+mhUsIEpic5b3l6oQlV3gMAXdlHQ40t4R2OezlaFIpiodZx0LF97BvDb0UEqIoP2yVeuNUjj14oYE5Ux5cpAyQ6KyWCqh3Hx+RM/2yZhhMHVKXMJO36DCstq6s59zBI0jKm62OqsgnjHnrJG7nntbBx+lLLxnzdwtEbilNkmmZFVao108X+fekmKpriRcbdzGQdjQsDjTsR/R2A1wC4jojmAPwWgCkiemu4y70APhS+PgTgc8RLzuYBvHHkI94EhHSqrjmYSgSuJInhtqNPRZVySVQrZiqDpuexZR+KzFOp9k2oALYhoCp5qBseTHcdr7j964WLbfIwWW1DkgKcXmjipv1c6VIdgtbJIqmjMayc67igKjYWGibvwjSmiVhRXNiu0lW4ldpH9jEzWce+mbWxjEEg22AjCJtj9Dt3kWHDuyQFqWCnON583YRa2fjEn6RlLDfAzCaMnvCShw2mAnGu+/HLtVAGongnNl21sVznzbDblo/DmfgU717Gx2ZFDWu22bgzxn6ix1vvzdn3EoAXbXJMYwMRz4bZv2e560G74fDs5o+tOVhp2Tiyh2vVbHVA9XnXXQUR8MKbni0UCOoHSWKYmmjj9EITdx7dD2CznvvOo2VUxcHcGp+4N2IMioDrC1UHLsFf/fKv9H1/JGPJ6A91Im39/p57wHiLvWwmi/jcesfFjVMbv36ClnE8H5YTbGryF+mQG1lJiGfmWNicvGhPAf5ZCy3Lx1JIzWTjUxW9jYtr/Hhb0T8VuIbkB4riW1/60NgeZFU1sdKy0LYnAWy9ETuwdxUH9q4O3rEgpibqOLXQyNXGHxbpgOrOoGU01YnapI3rtxLKoIPOedTVsXnI9j3NNivPQ7IRtuF6XbSMwGbiMWLF3HH8qEfrRiFomY04VsJz/9ol3vVqKM89nBjOLnJGIJvzP1HpYHbBDAsct4aWuWbkB4qioo+vvFtTLSw2TV7x2Kdr0m7B9EQTa203Ui3czAPMedXxGtJhkZysxkbLCOO+A6ioWMI3DqwC/c9daI4bjg8rk8mSPKeN5rgDMecu8sA349GqigtVdjdFy5xZbKKiuUOdk4i5PbvUCo+VflaqlQ68AFhsWrFGz3bTMiWKQ9csrNQcdBzeYm+3QwRVv3p+DbK0udQtLkIVwPPlHZEKCaQnq1Fq7ych4i4bLcwZJeLmG/x3LCJ1LH6rtu3ByAmoCmy0OhUAKExWEHLdm6Ur7rjlRETPDANF9iBJAYJAGloGQhQy9fbc+Xgur8WrgW2vUC1RHJpqo235WG07O8Y73QyEcX9moZkSDdsohNrfTrk2WiIXeVzBb2E4d4Jxj9vmpXXa+022SoKnt3NSIQU247kTGACGZtgDebNNLJ533QJmphrDj4OAisbPo0hlahKClunlufNWnMCVNaNLxmFcKI37CCF+4IurnR1RYr9ZaKqDqiZEkDYXoAUSmiQ7xLirW0LLDDagW4Vsk+witEyyG5OZqapMvt6U506ALLGIlhl3+7l+UMMeEEUEw5IQDXvOhbLN2etR0U1IFODyuhF1S9upFaolciB4tytrnaE6r+9kTE7w4JKyiTxmAeG9jivmMSyS2T/jymwS3O+4vbSiSDbJbheopE52Y+KpkL1omc1dP4lYRMuMu/1cP6jhMzyspr7w+h0v4KJhGU16Lo1g4cqaUQZUdyOE5+6MMW96qyGomVF47sK474TgIpDx3MdGywjPfWcY92SrvU5OP9u8/QGuCeT6LCUUJ4+Icwf4qkI0HN/OiVAEVYfJlIk+G/Lumpp/PXW9jUtr7S1LhSyN+wgh9GWAncMrbxbCuG8mDVIgMu475NooctyBqUgXpo19x87JlgHSsr9F2gvGuez8909ny/B+p8AoPPcgEVDdvmtV0UwQBahWhjfuqsprJtQezdonKh1cWTdgR8Z9vOdZZsuMEJrmgN/sNLZc+q1G5Llv0jMDYu91p0x8vPethyCgsaWtCsO5U6go0WADCOV+qbcsAhAboLVQHydJJXB5hQBBQF00xLAgKYhTIbfxWn3T9Rdx3b6VwlLWSQhaNtu7WWCi0sGs5WOhwd+XNnnNBqE07iOERAwVzYPlqDvGgG0Wk1WuUTNsp6M8cPmHYMPSsOOAqtjwg/E9BrHnvlOMe9xZqWN7UAa0F5SkAEQMq6HOedbwKrIPyJv/PSViUUHZ9tIyDjS1v0hgLwhatpcjJNIhn11shavF8T4HpXEfMTTVguWoO4Z62CyIgJfdemIkx5Ilf8ddF0WxAE8f3/F3ULYMkG6S3bI9qAPGRcQVFkWHoqzhlWWvcJP0/t8Te7HjDjSOC3rEuecbd0H1nFlsbbgJ+DAojfuIwXm36WvGcx8lDuxd3jFGTuCFN52FH8hjO7540LdaRK4XUgFV2yv0eyiyH3UoyhpeWXJHkvabNO47hcIaFiIY28tzF9IIq20bVb007rsOYvbeatGw3YDrD83j+kPz2z2MFPbvGa8SY0W38Ko7HsX+MSs+FoUi+2h34jz3Iu0FZdnDWiffc7/l5mci4a/NgMLj8jZ+4zd844B49nvJdCiyj6rmwHS0LaGeSuM+Yujh7F167iUE+jVW32rIiVaMLcst5LlLkpubLQMA1+0bTYtMgqhe7i2NvNMxWe1AVRzMTPSWLqhU2jCd/VtSsVwa9xEj8tx3yDK8RIkkkk2y2wX72cqyGwU7x1VgJLz1zWbdbCc01cH3fPtn++5T1TuoYWuM+8A8dyL6ayJaJqKnE9teTkSPENFTRPRJIpoJt6tEdFe4/TQR/eo4B78TES3NrpFUyBLXFpJNsrlxH3yfdue2jx6C2tlpMZlRQwRVd4RxB/BhAN+X2fYBAO9kjL0MwD8B+JVw+48C0MPt3wLg/yKioyMZ6S7Bof3/f3t3HyNXVcZx/Pvbbl+gL7biligUWyJbgmIrbrTGN0BDqhirQZSVqAlNGoKJr8HUWGP0PxOj1sTYNJWiotWIoqBGU1GzxpSXLVZoYeVVoULZRdHiW1vx8Y97pgzrTHd2du7O7rm/TzKZO/femXlOT/v0zLnnnnOItWffxqIT/DQz65Zn5md/ehIXVOtWXyorudda7rN0pEyrji8mMg3lnDC5R8QQMH7gZz9QWzpmN3BJ7XRgoaRe4CTgKFCpLNfTE5x6yqFZ229oeXvWFL5HTrzE3vj3QHnDFGtL7WmG3MlblpOPt9xnQHJv4gCwIW1fCqxI29cD/wAeAx4GPhcR7d0RYGYd15uS5xN/LxaDbiW5T0vLPXXL9Cjv5H7S8ZZ7+dcW2k3uVwBXSdoLLKZooQO8AngaeAGwCviopDMbfYCkTZKGJQ2PjXXmiruZnVjtDtPRp1of1VV/V2pZt8wf75aZpWPcWzVv7hF621wparLaGi0TESPARQCS+oGL06F3Az+NiGPAqKTfAAPAgw0+YzuwHWBgYGDm3I9ulrFaF8vo4X8/6/WJ1P4DKPOWeVWkz12Cl59zCwvm/6v072qr5S5peXruAbYA29Khh4EL07GFwDpgZOphmlkn1BL144dbH7Jb+w+gzFvma/MN5T5aBmDpkidZML/x5GKd1MpQyF3AHmC1pIOSNgKDku6lSNyPAjvT6V8GFkk6ANwO7IyIO8sJ3cwm63jLfVLdMrX5ccpL7lVpuU+nCbtlImKwyaGtDc79O8UFVjObgWrj3EcPp+kEWumW6akt8l1e4q3KUMjp5MU6zCqkNj/740/V5kCaRMu9xMSr2mgZJ/eOcXI3q5DaItm1lntrQyHTpF4ljkGvXajNfbTMdHJyN6uYuXOePj7t72Ra7mWOQXe3TOc5uZtVzPGWONFSN8h0LPKtioxzn05O7mYVU2uJz51gib3x55d6QbU2cVjm0w9MJyd3s4qprZzU29tasq61pku9oKrur5+aGyd3s4qptY7ntLg8Xo+K7psyE29Vph+YTk7uZhXzzE1Jrc9vsnzZIZYtKW8OQA+F7DyvxGRWMe0k9zVn7y0rHMCjZcrglrtZxdRGy8ykeVxOWfoEK0+7n0ULn+p2KNlwy92sYmp97jNpEfd5c4+yeuXd3Q4jK265m1XMM7M8zpzkbp3n5G5WMb1O7pXg5G5WMXNmYJ+7dZ6Tu1nFuFumGpzczSqmdwZeULXOa2UlpmskjUraX7dvjaQ9ku6SdJOkJWn/5ZL21T3+K2ltmQUws8lxy70aWmm5XwusH7dvB7A5Is4FbgCuBoiIb0bE2ohYC7wHeCgi9nUwXjObosULD3PK0lGWLHqy26FYiSZM7hExBIy/77gfGErbu4FLGrx1EPj2lKIzs46bN/cYAy++hQXzj3Q7FCtRu33uB4ANaftSYEWDc94F7Gr2AZI2SRqWNDw2NtZmGGZm1ki7yf0K4CpJe4HFwNH6g5JeCfwzIvY3ejNARGyPiIGIGOjr62szDDMza6St6QciYgS4CEBSP3DxuFMu4wStdjMzK1dbyV3S8ogYldQDbAG21R3rAd4JvLYzIZqZ2WS1MhRyF7AHWC3poKSNwKCke4ER4FFgZ91bXgc8EhEPlhGwmZlNbMKWe0QMNjm0tcn5vwLWTSEmMzObIt+hamaWISd3M7MMObmbmWXIyd3MLENO7mZmGXJyNzPLkJO7mVmGnNzNzDLk5G5mliEndzOzDDm5m5llyMndzCxDTu5mZhlycjczy5CTu5lZhpzczcwy1MpKTNdIGpW0v27fGkl7JN0l6SZJS+qOvTQdO5COLygreDMza6yVlvu1wPpx+3YAmyPiXOAG4GoASb3AdcCVEfFi4HzgWKeCNTOz1kyY3CNiCPjLuN39wFDa3g1ckrYvAu6MiN+l9/45Ip7uUKxmZtaidvvcDwAb0valwIq03Q+EpJ9JukPSx5p9gKRNkoYlDY+NjbUZhpmZNdJucr8CuErSXmAxcDTt7wVeA1yent8u6Q2NPiAitkfEQEQM9PX1tRmGmZk10tvOmyJihKILBkn9wMXp0EFgKCKeSMd+ApwH3Dz1UM3MrFVttdwlLU/PPcAWYFs69DPgXEknp4urrwfu7kSgZmbWulaGQu4C9gCrJR2UtBEYlHQvMAI8CuwEiIgngc8DtwP7gDsi4sdlBW9mZo1N2C0TEYNNDm1tcv51FMMhzcysS3yHqplZhpzczcwy5ORuZpYhJ3czsww5uZuZZcjJ3cwsQ07uZmYZcnI3M8uQk7uZWYac3M3MMuTkbmaWISd3M7MMObmbmWXIyd3MLENO7mZmGXJyNzPLUCsrMV0jaVTS/rp9ayTtkXSXpJskLUn7V0r6l6R96bGt+SebmVlZWmm5XwusH7dvB7A5Is4FbgCurjv2QESsTY8rOxOmmZlNRivL7A1JWjludz8wlLZ3UyyM/cmORtaiuT1zWTRvUTe+2sxsShbOW1jaZ0+Y3Js4AGwAfgBcCqyoO7ZK0m+Bw8CWiPh1ow+QtAnYBHDGGWe0GQZcsOoCLlh1QdvvNzPLUbsXVK8ArpK0F1gMHE37HwPOiIiXAR8BvlXrjx8vIrZHxEBEDPT19bUZhpmZNdJWyz0iRoCLACT1Axen/UeAI2l7r6QHKLpwhjsSrZmZtaStlruk5em5B9gCbEuv+yTNSdtnAmcBD3YmVDMza9WELXdJu4DzgedJOgh8Clgk6f3plO8DO9P264DPSDoG/Be4MiL+0vGozczshFoZLTPY5NDWBud+D/jeVIMyM7Op8R2qZmYZcnI3M8uQk7uZWYac3M3MMqSI6HYMSBoD/jiFj3ge8ESHwpktqlhmqGa5XebqmGy5XxgRDe8CnRHJfaokDUfEQLfjmE5VLDNUs9wuc3V0stzuljEzy5CTu5lZhnJJ7tu7HUAXVLHMUM1yu8zV0bFyZ9HnbmZmz5ZLy93MzOo4uZuZZWhWJ3dJ6yX9XtL9kjZ3O54ySFoh6ZeS7pZ0QNIH0/7nStot6b70vKzbsZZB0hxJv5X0o/R6laRbU51/R9K8bsfYSZKWSrpe0oikeyS9qgp1LenD6e/3fkm7JC3Isa4lXSNpVNL+un0N61eFL6Xy3ynpvMl816xN7mne+C8DbwLOAQYlndPdqErxH+CjEXEOsA54fyrnZuDmiDgLuDm9ztEHgXvqXn8W+EJEvAh4EtjYlajKsxX4aUScDayhKHvWdS3pNOADwEBEvASYA1xGnnV9LbB+3L5m9fsmijUxzqJYkvQrk/miWZvcgVcA90fEgxFxFPg2xbquWYmIxyLijrT9FMU/9tMoyvq1dNrXgLd1J8LySDqdYpWvHem1gAuB69MpWZVb0nMo1kT4KkBEHI2Iv1KBuqaYfvwkSb3AyRRLdmZX1xExBIxf46JZ/W4Avh6FW4Clkp7f6nfN5uR+GvBI3euDaV+2JK0EXgbcCpwaEY+lQ4eAU7sUVpm+CHyMYuEXgFOAv0bEf9Lr3Op8FTAG7ExdUTskLSTzuo6IPwGfAx6mSOp/A/aSd13Xa1a/U8pxszm5V4qkRRQLoXwoIg7XH4tiPGtWY1olvQUYjYi93Y5lGvUC5wFfSYvM/4NxXTCZ1vUyilbqKuAFwEL+v+uiEjpZv7M5uf8JWFH3+vS0LzuS5lIk9m9GxPfT7sdrP9HS82i34ivJq4G3SvoDRZfbhRT90UvTT3fIr84PAgcj4tb0+nqKZJ97Xb8ReCgixiLiGMXSna8m77qu16x+p5TjZnNyvx04K11Rn0dxAebGLsfUcamf+avAPRHx+bpDNwLvS9vvA3443bGVKSI+HhGnR8RKirr9RURcDvwSeEc6LatyR8Qh4BFJq9OuNwB3k3ldU3THrJN0cvr7Xit3tnU9TrP6vRF4bxo1sw74W133zcQiYtY+gDcD9wIPAJ/odjwllfE1FD/T7gT2pcebKfqfbwbuA34OPLfbsZb4Z3A+8KO0fSZwG3A/8F1gfrfj63BZ1wLDqb5/ACyrQl0DnwZGgP3AN4D5OdY1sIviusIxil9qG5vVLyCKEYEPAHdRjCZq+bs8/YCZWYZmc7eMmZk14eRuZpYhJ3czsww5uZuZZcjJ3cwsQ07uZmYZcnI3M8vQ/wDgq5aZE5nhTgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_kCnsPUqS6o"
      },
      "source": [
        "You can import your own data into Colab notebooks from your Google Drive account, including from spreadsheets, as well as from Github and many other sources. To learn more about importing data, and how Colab can be used for data science, see the links below under [Working with Data](#working-with-data)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwuxHmxllTwN"
      },
      "source": [
        "## Machine learning\n",
        "\n",
        "With Colab you can import an image dataset, train an image classifier on it, and evaluate the model, all in just [a few lines of code](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/quickstart/beginner.ipynb). Colab notebooks execute code on Google's cloud servers, meaning you can leverage the power of Google hardware, including [GPUs and TPUs](#using-accelerated-hardware), regardless of the power of your machine. All you need is a browser."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufxBm1yRnruN"
      },
      "source": [
        "Colab is used extensively in the machine learning community with applications including:\n",
        "- Getting started with TensorFlow\n",
        "- Developing and training neural networks\n",
        "- Experimenting with TPUs\n",
        "- Disseminating AI research\n",
        "- Creating tutorials\n",
        "\n",
        "To see sample Colab notebooks that demonstrate machine learning applications, see the [machine learning examples](#machine-learning-examples) below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Rh3-Vt9Nev9"
      },
      "source": [
        "## More Resources\n",
        "\n",
        "### Working with Notebooks in Colab\n",
        "- [Overview of Colaboratory](/notebooks/basic_features_overview.ipynb)\n",
        "- [Guide to Markdown](/notebooks/markdown_guide.ipynb)\n",
        "- [Importing libraries and installing dependencies](/notebooks/snippets/importing_libraries.ipynb)\n",
        "- [Saving and loading notebooks in GitHub](https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb)\n",
        "- [Interactive forms](/notebooks/forms.ipynb)\n",
        "- [Interactive widgets](/notebooks/widgets.ipynb)\n",
        "- <img src=\"/img/new.png\" height=\"20px\" align=\"left\" hspace=\"4px\" alt=\"New\"></img>\n",
        " [TensorFlow 2 in Colab](/notebooks/tensorflow_version.ipynb)\n",
        "\n",
        "<a name=\"working-with-data\"></a>\n",
        "### Working with Data\n",
        "- [Loading data: Drive, Sheets, and Google Cloud Storage](/notebooks/io.ipynb) \n",
        "- [Charts: visualizing data](/notebooks/charts.ipynb)\n",
        "- [Getting started with BigQuery](/notebooks/bigquery.ipynb)\n",
        "\n",
        "### Machine Learning Crash Course\n",
        "These are a few of the notebooks from Google's online Machine Learning course. See the [full course website](https://developers.google.com/machine-learning/crash-course/) for more.\n",
        "- [Intro to Pandas](/notebooks/mlcc/intro_to_pandas.ipynb)\n",
        "- [Tensorflow concepts](/notebooks/mlcc/tensorflow_programming_concepts.ipynb)\n",
        "- [First steps with TensorFlow](/notebooks/mlcc/first_steps_with_tensor_flow.ipynb)\n",
        "- [Intro to neural nets](/notebooks/mlcc/intro_to_neural_nets.ipynb)\n",
        "- [Intro to sparse data and embeddings](/notebooks/mlcc/intro_to_sparse_data_and_embeddings.ipynb)\n",
        "\n",
        "<a name=\"using-accelerated-hardware\"></a>\n",
        "### Using Accelerated Hardware\n",
        "- [TensorFlow with GPUs](/notebooks/gpu.ipynb)\n",
        "- [TensorFlow with TPUs](/notebooks/tpu.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-H6Lw1vyNNd"
      },
      "source": [
        "<a name=\"machine-learning-examples\"></a>\n",
        "\n",
        "## Machine Learning Examples\n",
        "\n",
        "To see end-to-end examples of the interactive machine learning analyses that Colaboratory makes possible, check out these  tutorials using models from [TensorFlow Hub](https://tfhub.dev).\n",
        "\n",
        "A few featured examples:\n",
        "\n",
        "- [Retraining an Image Classifier](https://tensorflow.org/hub/tutorials/tf2_image_retraining): Build a Keras model on top of a pre-trained image classifier to distinguish flowers.\n",
        "- [Text Classification](https://tensorflow.org/hub/tutorials/tf2_text_classification): Classify IMDB movie reviews as either *positive* or *negative*.\n",
        "- [Style Transfer](https://tensorflow.org/hub/tutorials/tf2_arbitrary_image_stylization): Use deep learning to transfer style between images.\n",
        "- [Multilingual Universal Sentence Encoder Q&A](https://tensorflow.org/hub/tutorials/retrieval_with_tf_hub_universal_encoder_qa): Use a machine learning model to answer questions from the SQuAD dataset.\n",
        "- [Video Interpolation](https://tensorflow.org/hub/tutorials/tweening_conv3d): Predict what happened in a video between the first and the last frame.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YG4swMX4AzF4",
        "outputId": "fc0027fa-0db2-47e6-80c0-e13bec3887ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "#libraries used\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import unittest #for testing\n",
        "\n",
        "\"\"\"\n",
        "I should refactor code to not have multiple import statements and only have them here\n",
        "\"\"\""
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nI should refactor code to not have multiple import statements and only have them here\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoaGx4yZGX3U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "5969ee88-117b-4ec2-8a14-dbd2ff55c543"
      },
      "source": [
        "string = 'Peter picked a pail of pickled peppers'\n",
        "string"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Peter picked a pail of pickled peppers'"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lT17_nWX4YR9"
      },
      "source": [
        "globalGroupID=\"\"\n",
        "\n",
        "class DebugLog:\n",
        "  def __init__(self, groupsToPrint : list) -> None:\n",
        "    self.__groupsToPrint = groupsToPrint\n",
        "  \n",
        "  def print(self, message : str,  groupID = globalGroupID) -> None:\n",
        "    if(groupID in self.__groupsToPrint):\n",
        "      print(message)\n",
        "  \n",
        "  def set_groupsToPrint(groupsToPrint : list) -> None :\n",
        "    self.__groupsToPrint = groupsToPrint\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKuRkg9w-mLs"
      },
      "source": [
        "def create_global_debugger(groupsToPrint : list) -> DebugLog:\n",
        "  globalDL = DebugLog()\n",
        "  return globalDL"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alrCH4Us5scf"
      },
      "source": [
        "\n",
        "def try_global_debugger():\n",
        "  groupsToPrint = [\"g1\", \"collectHistories\"]\n",
        "  globalDL = create_global_debugger(groupsToPrint)\n",
        "  globalGroupID = \"g1\"\n",
        "  globalDL.print(\"I want to print these messages\", globalGroupID)\n",
        "  globalGroupID = \"notAGroupToPrint\" #arbitrary ID\n",
        "  globalDL.print(\"I don't want to print thesse messages anymore\", globalGroupID)\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVNvKnFNUsFE"
      },
      "source": [
        "# This is the first code you should start from\n",
        "import tensorflow as tf\n",
        "\n",
        "def try_tensorflow():\n",
        "  print(tf.__version__)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTEF949BBo0K"
      },
      "source": [
        "# Our data are stored in GITHUB, goto  https://github.com/bsuwpc/DSP click correct pure normalized data.csv, then click RAW. Copy its link address and paste to URL below, that link address of RAW data is temporary (expired in 5 minutes)\n",
        "class UrlManager:\n",
        "  def __init__(self) -> None:\n",
        "    self.__newDataUrl = 'https://raw.githubusercontent.com/bsuwpc/DSP/main/2021_11_16_processedData.csv?token=GHSAT0AAAAAABM7URQZI53XK5OLMWO6KDIMYPPKIFQ'\n",
        "    self.__originalDataUrl = 'https://raw.githubusercontent.com/bsuwpc/DSP/main/correct%20pure%20normalized%20data.csv?token=ARTBCILKD2QF5SZYETYABMDBSL64Y' #For clarifying what data was tested before; url still have to updated\n",
        "  \n",
        "  def get_newDataUrl(self) -> str:\n",
        "    return self.__newDataUrl\n",
        "\n",
        "  def get_originalDataUrl(self) -> str:\n",
        "    return self.__originalDataUrl \n",
        "\n",
        "  \n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d97Vu26qCLDZ"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "def original_read_csv(url : str) -> pd.DataFrame: \n",
        "  data = pd.read_csv(url)\n",
        "  return data\n",
        "# If some error, redo above code to get data from GITHUB\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1Xr1vKYDfVe"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "def original_print_head(data : pd.DataFrame)  -> pd.DataFrame:\n",
        "  print(data.head())\n",
        "# compare this result with our normalized data to confirm."
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAPQzLvOQVPH"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "def original_print_info(data : pd.DataFrame ) -> None:\n",
        "   print(data.info())\n",
        "# for confirmation"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqKr54chB2al"
      },
      "source": [
        "def removeCols(data):\n",
        "    emptyCol= 'Unnamed: 20'\n",
        "    nonAdjustedExecutionTime = 'execution time'\n",
        "    data = data.drop(emptyCol, axis=1)\n",
        "    data = data.drop(nonAdjustedExecutionTime, axis=1)\n",
        "    return data\n",
        "\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2Eqn_KE4KmU"
      },
      "source": [
        "def preprocessData(data):\n",
        "  data = removeCols(data)\n",
        "  return data"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vq4s6wcpPsVD"
      },
      "source": [
        "import pandas as  pd\n",
        "\n",
        "def describe_data(data: pd.DataFrame) -> pd.DataFrame:\n",
        "  data_t = data.describe()\n",
        "  desc = data_t.T\n",
        "  desc\n",
        "  return desc\n",
        "# count = 33, which means our data contain 33 samples "
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiscujXcwPFo"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "def show_attribute_stats(desc : pd.DataFrame) -> pd.DataFrame:\n",
        "  return desc[['mean', 'std']]\n",
        "# this results show that all features (attributes in uur papers) vary over a wide range"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bg3ZsVCheXWK"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "def get_cols(data : pd.DataFrame)-> list: \n",
        "  cols = list(data)\n",
        "  cols\n",
        "  return cols\n",
        "# this code is not necessary"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzycqBrN_HVW"
      },
      "source": [
        "# create a copy of the DataFrame\n",
        "import pandas as pd\n",
        "\n",
        "urlToAttribute = {\n",
        "    \"newDataUrl\": 'adjusted execution time',\n",
        "    \"url\"  :'execution time'\n",
        "}\n",
        "\n",
        "\n",
        "def choose_attribute_name(urlName : str ) -> str :\n",
        "  return urlToAttribute[urlName]\n",
        "\n",
        "  \n",
        "def get_target(data : pd.DataFrame, url : str) -> pd.DataFrame:\n",
        "  df = data.copy()\n",
        "  # create the target\n",
        "  targetAttributeName=''\n",
        "  if(url == newDataUrl):\n",
        "    targetAttributeName='adjusted execution time'\n",
        "  else:\n",
        "    targetAttributeName='execution time'\n",
        "\n",
        "    \n",
        "  target = df.pop(targetAttributeName)\n",
        "  print (target.head())\n",
        "  return target\n",
        "# execution time is the target of prediction, using df.pop to create the target set; rest of df are feature sets. target.head shows the first 5 samples' target values\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGOJJjHJ_nyK"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "def print_head(df: pd.DataFrame)-> None:\n",
        "  print(df.head())   \n",
        "# check the first five feature sets"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbP72T5o_vL3"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "def get_feature_cols(df : pd.DataFrame) -> list:\n",
        "  feature_cols = list(df)\n",
        "  return feature_cols\n",
        "\n",
        "# check names of features"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "767m3hSi_-sK"
      },
      "source": [
        "def get_length_of_feature_col(feature_cols : list):\n",
        "  len(feature_cols)\n",
        "# check total number of features"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGnBY0nFADfo"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as  pd\n",
        "\n",
        "PureDataManagerFactory_cnt = 0\n",
        "\n",
        "class PureDataManager:\n",
        "  def __init__(self , pureDataForTraining : dict) -> None:\n",
        "    self.__pureDataForTraining = pureDataForTraining\n",
        "    self.__keyToFeatureData = \"features\"\n",
        "    self.__keyToLabelData  = \"labels\"\n",
        "\n",
        "  def get_features(self):\n",
        "    self.__pureDataForTraining[self.__keyToFeatureData]\n",
        "  \n",
        "  def get_labels(self):\n",
        "    self.__pureDataForTraining[self.__keyToFeatureData]\n",
        "\n",
        "\n",
        "class PureDataManagerFactory:\n",
        "  def __init__(self, object_from_this_class_cnt = PureDataManagerFactory_cnt) -> None  :\n",
        "    self.__object_cnt = object_from_this_class_cnt\n",
        "    if(self.__object_cnt > 1):\n",
        "      raise Exception(\"Only one object for this class\")\n",
        "    else:\n",
        "      PureDataManagerFactory_cnt+=1\n",
        "\n",
        "  def make_pure_data_manager(self, featureDf : pd.DataFrame, targetDf: pd.DataFrame  ) -> PureDataManager:\n",
        "    features = featureDf.values\n",
        "    labels = targetDf.values\n",
        "\n",
        "    puredata = {\n",
        "        \"features\": features,\n",
        "        \"labels\": labels\n",
        "    }\n",
        "    \n",
        "    return PureDataManager(pureData)\n",
        "\n",
        "  def __del__(self) -> None:\n",
        "    PureDataManagerFactory_cnt -= 1\n",
        "    \n",
        "\n",
        "def try_pure_data_manager(df : pd.DataFrame, target : pd.DataFrame) -> None:\n",
        "  factory = PureDataManagerFactory()\n",
        "  puredata = factory.make_pure_data_manager(df , target)\n",
        "  \n",
        "  print(puredata.get_features())\n",
        "  print(puredata.get_labels())\n",
        "  print(type(features), type(labels))\n",
        "# features and labels are pure data, all named are removed"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioRh061XANom"
      },
      "source": [
        "import pandas as pd\n",
        "def print_pure_data(df : pd.DataFrame, target: pd.DataFrame ) -> None:\n",
        "  features = df.values\n",
        "  labels = target.values\n",
        "  print(features)\n",
        "  print\n",
        "  print(labels)\n",
        "# compare results with \"correct pure normalized data \" xlxs or csv file"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMwHGManA1ZO"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "class InputVariablesManager: \n",
        "  def __init__(self, input_variables : dict ) -> None :\n",
        "    self.__input_variables =  input_variables\n",
        "    self.__key_to_train_input = \"X_train\"\n",
        "    self.__key_to_validation_input = \"X_test\"\n",
        "    \n",
        "  def get_train_input_variables(self) -> np.ndarray:\n",
        "    return self.__input_variables[self.__key_to_train_input]\n",
        "  \n",
        "  def get_validation_input_variables(self) -> np.ndarray:\n",
        "    return self.__input_variables[self.__key_to_validation_input]\n",
        "\n",
        "class InputVariablesCreator:\n",
        "  def __init__(self, features : np.ndarray, num_of_slices = 25) -> None:\n",
        "    self.__features = features \n",
        "    self.__num_of_slices = num_of_slices\n",
        "\n",
        "  def seperate_training_and_testing_inputs(self) -> InputVariablesManager:\n",
        "    X_train, X_test = np.vsplit(self.__features,[self.__num_of_slices])\n",
        "    input_variables = {\n",
        "        \"X_train\" : X_train,\n",
        "        \"X_test\": X_test\n",
        "    }\n",
        "\n",
        "    input_variables = InputVariablesManager(input_variables)\n",
        "    return input_variables\n",
        "  \n",
        "\n",
        "# separate training set and testing set of features, the first 25 samples are for training"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdUxuLlAEUKx"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "class TargetVariablesManager: \n",
        "  def __init__(self, target_variables : dict ) -> None :\n",
        "    self.__target_variables =  target_variables\n",
        "    self.__key_to_train_target = \"y_train\"\n",
        "    self.__key_to_validation_target = \"y_test\"\n",
        "\n",
        "  def get_train_target_variables(self) -> np.ndarray:\n",
        "    return self.__target_variables[self.__key_to_train_target]\n",
        "  \n",
        "  def get_validation_target_variables(self) -> np.ndarray:\n",
        "    return self.__target_variables[self.__key_to_validation_target]\n",
        "\n",
        "\n",
        "class TargetVariablesCreator:\n",
        "  def __init__(self, labels : np.ndarray, num_of_slices = 25) -> None:\n",
        "    self.__labels = labels\n",
        "    self.__num_of_slices = num_of_slices\n",
        "  \n",
        "  def seperate_training_and_testing_targets(self) -> TargetVariablesManager:\n",
        "    y_train, y_test = np.split(self.__labels,[self.__num_of_slices])\n",
        "    target_variables ={\n",
        "        \"y_train\": y_train,\n",
        "        \"y_test\" : y_test\n",
        "    }\n",
        "\n",
        "    return TargetVariablesManager(target_variables)\n",
        "\n",
        "\n",
        "# separate training set and testing set of lables. Please notive case semsitive."
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OL48uJjhC-k1"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def print_input_variables(X_train : np.ndarray , X_test : np.ndarray) ->  None :\n",
        "  br='\\n'\n",
        "  print ('X_train:', end=' ')\n",
        "  print (X_train, br)\n",
        "  print ('X_test:', end=' ')\n",
        "  print (X_test)\n",
        "# print training set and testing set of features, compare results with \"correct pure normalized data \" xlxs or csv file"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JU3UBy8DY0Y"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def print_input_variables(y_train: np.ndarray, y_test: np.ndarray) -> None:\n",
        "  br='\\n'\n",
        "  print ('y_train:', end=' ')\n",
        "  print (y_train, br)\n",
        "  print ('y_test:', end=' ')\n",
        "  print (y_test)\n",
        "# print training set and testing set of labels, compare results with \"correct pure normalized data \" xlxs or csv file"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nb97BBMFFJJ2"
      },
      "source": [
        "\"\"\"\n",
        "the result of test_bs must come from the train or test object\n",
        "\n",
        "tensor slices ((input data_std -> target val))\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "# scale feature data and create TensorFlow tensors\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "#wrapper to hold old code while being functional\n",
        "def create_datasets(X_train: list , X_test:list,  y_train : list, y_test : list ) -> dict: \n",
        "  scaler = StandardScaler()\n",
        "  X_train_std = scaler.fit_transform(X_train)\n",
        "  X_test_std = X_test\n",
        "  train = tf.data.Dataset.from_tensor_slices( (X_train_std, y_train))\n",
        "  test = tf.data.Dataset.from_tensor_slices( (X_test_std, y_test))\n",
        "  \n",
        "  dataSets = {\n",
        "      \"train\": train,\n",
        "      \"test\" : test\n",
        "  }\n",
        "  return dataSets \n",
        "\n",
        "# train and test are TF tensors of training and testing sets separately, each item contains feature and lable two components\n",
        "\n",
        "def get_training_set(dataSets: dict, keyToTrainingSet = \"train\") -> list:\n",
        "  return dataSets[keyToTrainingSet]\n",
        "\n",
        "def get_testing_set(dataSets: dict, keyToTrainingSet = \"test\"):\n",
        "  return dataSets[keyToTrainingSet]\n",
        "  \n",
        "\n"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConverterToDatasetBusinessInterface:\n",
        "  def convert(self) -> tf.data.Dataset:\n",
        "    pass"
      ],
      "metadata": {
        "id": "NJG6gRYs9HZG"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "class TrainingDataConverterToTensorDatasetBusiness(ConverterToDatasetBusinessInterface):\n",
        "  def __init__(self, inputDataToTransform : list, targetDataToTransform : list, scaler = StandardScaler()\n",
        ") -> None :\n",
        "    self.__inputDataToTransform = inputDataToTransform\n",
        "    self.__targetDataToTransform = targetDataToTransform \n",
        "    self.__scaler = scaler\n",
        "  \n",
        "  def convert(self) -> tf.data.Dataset:\n",
        "    inputDataScaledToStandard = self.__scaler.fit_transform(self.__inputDataToTransform)\n",
        "    tensorDataset = tf.data.Dataset.from_tensor_slices((inputDataScaledToStandard, self.__targetDataToTransform)) \n",
        "    return tensorDataset\n",
        "  \n",
        "\n",
        "    \n",
        "    \n",
        "     "
      ],
      "metadata": {
        "id": "T4ALAcn7NeS7"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class ValidationDataConvertToTensorDatasetBusiness(ConverterToDatasetBusinessInterface):\n",
        "  def __init__(self, inputDataToTransform:list, targetDataToTransform:list) -> None :\n",
        "    self.__inputDataToTransform= inputDataToTransform\n",
        "    self.__targetDataToTransform = targetDataToTransform \n",
        "    \n",
        "  def convert(self) -> tf.data.Dataset:\n",
        "    tensorDataset = tf.data.Dataset.from_tensor_slices((self.__inputDataToTransform, self.__targetDataToTransform))\n",
        "    return tensorDataset\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "zc6pGt0762zb"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TestConstants:\n",
        "  def __init__(self):\n",
        "    self.__EXAMPLE_DATASETS = {\n",
        "      \"features\"  : tf.constant([[1, 3], [2, 1], [3, 3]]),\n",
        "      \"labels\" : tf.constant(['A', 'B', 'A'])\n",
        "    } \n",
        "\n",
        "  def get_EXAMPLE_DATASETS(self) -> dict:\n",
        "    return self.__EXAMPLE_DATASETS\n",
        "\n",
        "  def get_EXAMPLE_DATASETS_val(self, key: str ) -> any: #broken method. 1 argument, expected two error but self is here\n",
        "    return self.__EXAMPLE_DATASETS[key] #because constant?"
      ],
      "metadata": {
        "id": "WygyJBA5wAQ3"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SINGLETON_TestConstants = TestConstants()"
      ],
      "metadata": {
        "id": "NV-bp53Mw5-T"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import unittest\n",
        "import tensorflow as tf\n",
        "# scale feature data and create TensorFlow tensors\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "class TestConverterToTensorDatasetBusiness(unittest.TestCase):\n",
        "  def __init__(self) -> None:\n",
        "    features = SINGLETON_TestConstants_OBJ.get_EXAMPLE_DATASETS()[\"features\"]\n",
        "    labels = SINGLETON_TestConstants_OBJ.get_EXAMPLE_DATASETS()[\"labels\"] \n",
        "\n",
        "    self.__given_training_input_data =  features\n",
        "    self.__given_training_target_data = labels\n",
        "    self.__given_test_input_data = features\n",
        "    self.__given_test_target_data = labels\n",
        "  \n",
        "  def __original_code_to_tensor_dataset(self) -> dict:\n",
        "    dataSets = create_datasets(self.__given_training_input_data, self.__given_test_input_data, self.__given_training_target_data, self.__given_test_target_data)\n",
        "    return dataSets\n",
        "\n",
        "  \n",
        "  def __refactored_code_to_tensor_dataset(self) -> dict:\n",
        "    training_data_converter = TrainingDataConverterToTensorDatasetBusiness(self.__given_training_input_data, self.__given_training_target_data)\n",
        "    validation_data_converter = ValidationDataConvertToTensorDatasetBusiness(self.__given_test_input_data, self.__given_test_target_data)\n",
        "\n",
        "    training_dataset = training_data_converter.convert()\n",
        "    validation_dataset = validation_data_converter.convert()\n",
        "    dataSet = {\n",
        "        \"train\" : training_dataset,\n",
        "        \"test\": validation_dataset\n",
        "    }\n",
        "    return dataSet\n",
        "\n",
        "  def test_converter(self) -> None:\n",
        "    \n",
        "    originalRes = self.__original_code_to_tensor_dataset()\n",
        "    refactoredRes = self.__refactored_code_to_tensor_dataset()\n",
        "    \n",
        "    comparable_original_result = self.__convert_to_value_to_compare(originalRes)\n",
        "    comparable_refactored_result = self.__convert_to_value_to_compare(refactoredRes)\n",
        "        \n",
        "    if(not self.__assert_equal('','')):\n",
        "      raise Exception(\"string representations (coverted with str()) of tensor data sets are not equal\")\n",
        "    else:\n",
        "      print(f\"{TestConverterToTensorDatasetBusiness.__name__} test case passed\")\n",
        "  \n",
        "  def __convert_to_value_to_compare(self, toCompareInAssert : tf.data.Dataset) -> str: \n",
        "    return str(toCompareInAssert)\n",
        "  \n",
        "  def __assert_equal(self, original_result : str,  refactored_result: str ) -> bool:\n",
        "    return original_result == refactored_result\n",
        "\n",
        "  def run(self) -> None:\n",
        "    self.test_converter()\n",
        "     \n"
      ],
      "metadata": {
        "id": "IVIyb7yFwmd2"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ns1mDp3LFdm6"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def original_print_datasets(train : tf.data.Dataset, test: tf.data.Dataset ) -> None:\n",
        "  print ('train:', end=' ')\n",
        "  print (train, br)\n",
        "  print ('test:', end=' ')\n",
        "  print (test)\n",
        "# can not show whole picture of train and test tensors."
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXdrtrfpFgf2"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def original_print_input_datasets_standard_scaled(X_train_std : tf.data.Dataset, X_test_std: tf.data.Dataset) -> None:\n",
        "  print ('X_train_std:', end=' ')\n",
        "  print (X_train_std, br)\n",
        "  print ('X_test_std:', end=' ')\n",
        "  print (X_test_std)\n",
        "# this code is not necessary"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1ycjxAGFwx6"
      },
      "source": [
        "def see_samples(data, num = 5):\n",
        "  for feat, targ in data.take(num):\n",
        "    print ('Features: {}'.format(feat), br)\n",
        "    print ('Target: {}'.format(targ))\n",
        "# using this code can show the first five tensors of training set"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXShDCoYG3CB"
      },
      "source": [
        "\n",
        "def original_dataset_process(train : tf.data.Dataset, test: tf.data.Dataset) -> dict:\n",
        "  BATCH_SIZE, SHUFFLE_BUFFER_SIZE  = 10, 10\n",
        "  train_bs = train.shuffle(\n",
        "      SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE).prefetch(1)\n",
        "  test_bs = test.batch(BATCH_SIZE).prefetch(1)\n",
        "\n",
        "  data_sets = {\n",
        "      \"train_bs\" : train_bs,\n",
        "      \"test_bs\" : test_bs\n",
        "  }\n",
        "\n",
        "  return data_sets\n",
        "\n",
        "# same funcs, names are just to illustrate purpose\n",
        "def get_train_bs(data_sets, keyToTrain = \"train_bs\") -> tf.data.Dataset:\n",
        "  return data_sets[keyToTrain]\n",
        "\n",
        "def get_test_bs(data_sets, keyToTrain = \"test_bs\") -> tf.data.Dataset:\n",
        "  return data_sets[keyToTrain]\n",
        "# this code is necessary even our small data set does not need batch and shuffle\n",
        "\n"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class DatasetProcessorInterface:\n",
        "  def process_data(self) -> tf.data.Dataset:\n",
        "    pass"
      ],
      "metadata": {
        "id": "ykxWAF2Murh8"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "BATCH_SIZE, SHUFFLE_BUFFER_SIZE  = 10, 10\n",
        "\n",
        "class TrainingDatasetProcessor(DatasetProcessorInterface):\n",
        "  def __init__(self, datasetToGiveModel:tf.data.Dataset, batch_size = BATCH_SIZE, shuffle_buffer_size = SHUFFLE_BUFFER_SIZE , prefetch_buffer_size = 1) -> None:\n",
        "    self.__datasetToGiveModel = datasetToGiveModel\n",
        "    self.__batch_size = batch_size \n",
        "    self.__shuffle_buffer_size = shuffle_buffer_size\n",
        "    self.__prefetch_buffer_size = prefetch_buffer_size\n",
        "  \n",
        "  def process_data(self) -> tf.data.Dataset:\n",
        "     shuffled_dataset = self.__shuffle_dataset()\n",
        "     batched_dataset = self.__convert_dataset_to_batches_to_be_used_in_a_training_iteration(shuffled_dataset)\n",
        "     processed_data = self.__prefetch_elements_from_dataset(batched_dataset)\n",
        "     return processed_data\n",
        "  \n",
        "  def __shuffle_dataset(self) -> tf.data.Dataset:\n",
        "    shuffled_dataset = self.__datasetToGiveModel.shuffle(self.__shuffle_buffer_size)\n",
        "    return shuffled_dataset\n",
        "\n",
        "  def __convert_dataset_to_batches_to_be_used_in_a_training_iteration(self, shuffled_dataset: tf.data.Dataset) -> tf.data.Dataset:\n",
        "    batched_dataset = shuffled_dataset.batch(self.__batch_size)\n",
        "    return batched_dataset\n",
        "  \n",
        "  def __prefetch_elements_from_dataset (self, batched_dataset: tf.data.Dataset) -> tf.data.Dataset:\n",
        "    prefetched_dataset = batched_dataset.prefetch(self.__prefetch_buffer_size)\n",
        "    return prefetched_dataset\n",
        "    "
      ],
      "metadata": {
        "id": "42AqIfm4rL7I"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "BATCH_SIZE, SHUFFLE_BUFFER_SIZE  = 10, 10\n",
        "\n",
        "\n",
        "class ValiationDatasetProcessor(DatasetProcessorInterface):\n",
        "  def __init__(self, datasetToGiveModel:tf.data.Dataset, batch_size = BATCH_SIZE , prefetch_buffer_size = 1) -> None:\n",
        "    self.__datasetToGiveModel = datasetToGiveModel\n",
        "    self.__batch_size = batch_size \n",
        "    self.__prefetch_buffer_size = prefetch_buffer_size\n",
        "  \n",
        "  def process_data(self) -> tf.data.Dataset:\n",
        "     batched_dataset = self.__convert_dataset_to_batches_to_be_used_in_a_training_iteration(self.__datasetToGiveModel)\n",
        "     processed_data = self.__prefetch_elements_from_dataset(batched_dataset)\n",
        "     return processed_data\n",
        "  \n",
        "\n",
        "  def __convert_dataset_to_batches_to_be_used_in_a_training_iteration(self, shuffled_dataset: tf.data.Dataset) -> tf.data.Dataset:\n",
        "    batched_dataset = shuffled_dataset.batch(self.__batch_size)\n",
        "    return batched_dataset\n",
        "  \n",
        "  def __prefetch_elements_from_dataset (self, batched_dataset: tf.data.Dataset) -> tf.data.Dataset:\n",
        "    prefetched_dataset = batched_dataset.prefetch(self.__prefetch_buffer_size)\n",
        "    return prefetched_dataset\n"
      ],
      "metadata": {
        "id": "NkNwza0wuo3f"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import unittest\n",
        "\n",
        "class TestDatasetProcessor(unittest.TestCase):\n",
        "  def __init__(self):\n",
        "    self.__given_training_dataset = self.__get_training_dataset()\n",
        "    self.__given_test_dataset =  self.__get_test_dataset()\n",
        "    self.__batch_size = 10\n",
        "    self.__shuffle_buffer_size = 10\n",
        "    self.__prefetch_buffer_size = 1\n",
        "\n",
        "  \n",
        "  def run(self):\n",
        "    self.test_process_data()\n",
        "     \n",
        "  def test_process_data(self):\n",
        "    original_datasets = self.__original_code()\n",
        "    refactored_datasets = self.__refactored_code()\n",
        "\n",
        "    comparable_original  = self.__convert_to_comparable(original_datasets)\n",
        "    comparable_refactored  = self.__convert_to_comparable(refactored_datasets)\n",
        "\n",
        "    self.__comparison_operation(comparable_original, comparable_refactored)\n",
        "\n",
        "    \n",
        "\n",
        "  def __convert_to_comparable(self, dataset: tf.data.Dataset) -> str:\n",
        "    return str(dataset)\n",
        "  \n",
        "  def __comparison_operation(self, original_datasets : str, refactored_datasets:str) -> None:\n",
        "    if(not original_datasets == refactored_datasets):\n",
        "      raise Exception(\"the string representations of the processed datasets are not equal\")\n",
        "    else: \n",
        "      print(f\"{TestDatasetProcessor.__name__} test case passed\")\n",
        "    \n",
        "\n",
        "  def __original_code(self) -> dict:\n",
        "    return original_dataset_process(self.__given_training_dataset, self.__given_test_dataset)\n",
        "\n",
        "  def __refactored_code(self) -> tf.data.Dataset:\n",
        "    processor = ValiationDatasetProcessor(self.__given_test_dataset)\n",
        "    proccessed_test_data = processor.process_data()\n",
        "\n",
        "    processor = TrainingDatasetProcessor(self.__given_training_dataset)\n",
        "    processed_training_data = processor.process_data()\n",
        "\n",
        "    data_sets = { \n",
        "        \"train_bs\": processed_training_data,\n",
        "        \"test_bs\" : proccessed_test_data\n",
        "    }\n",
        "\n",
        "    return data_sets\n",
        "  \n",
        "  def __get_test_dataset(self):\n",
        "    labels = SINGLETON_TestConstants_OBJ.get_EXAMPLE_DATASETS()[\"labels\"]\n",
        "    features = SINGLETON_TestConstants_OBJ.get_EXAMPLE_DATASETS()[\"features\"]\n",
        "\n",
        "    return ValidationDataConvertToTensorDatasetBusiness(features, labels).convert()\n",
        "  \n",
        "  def __get_training_dataset(self):\n",
        "    labels = SINGLETON_TestConstants_OBJ.get_EXAMPLE_DATASETS()[\"labels\"]\n",
        "    features = SINGLETON_TestConstants_OBJ.get_EXAMPLE_DATASETS()[\"features\"]\n",
        "\n",
        "\n",
        "    return TrainingDataConverterToTensorDatasetBusiness(features,labels ).convert()\n",
        "\n",
        "    \n"
      ],
      "metadata": {
        "id": "n4x68kswr6Ya"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jX-gasZcHZCs"
      },
      "source": [
        "import tensorflow as tf \n",
        "def print_batch_datasets(train_bs : tf.data.Dataset, test_bs : tf.data.Dataset) -> None:\n",
        "  print(train_bs)\n",
        "  print(test_bs)\n",
        "# train_bs and test_bs will be used in training model and validating later, please make sure the results must be correct."
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTXeeFr7ctSy"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def plot_history(history, limit1, limit2):\n",
        " \n",
        "  hist = pd.DataFrame(history.history)\n",
        "  hist['epoch'] = history.epoch\n",
        "  plt.figure()\n",
        "  plt.xlabel('epoch')\n",
        "  plt.ylabel('MAPE [MPG]')\n",
        "  plt.plot(hist['epoch'], hist['mean_absolute_percentage_error'],    label='Train Error')\n",
        "  plt.plot(hist['epoch'], hist['val_mean_absolute_percentage_error'],    label = 'Val Error')\n",
        "  plt.ylim([0, limit1])\n",
        "  plt.legend()\n",
        "  plt.title('MAPE by Epoch')\n",
        "  plt.savefig(\"abc.png\")\n",
        "  plt.show()\n",
        "  return plt\n",
        "\n",
        "  \n",
        "  \n",
        "# set limits to make plot readable\n",
        "mean_absolute_percentage_error_limit, mae_limit = 200,200\n",
        "#plot_history(history, mean_absolute_percentage_error_limit, mae_limit)\n",
        "\n",
        "# change limit value can change the scale of vertical axis"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leu4UVcor8wO"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def original_copy_data_set_for_prediction(test_bs : tf.data.Dataset ) -> tf.data.Dataset:\n",
        "  test_n=test_bs\n",
        "  return test_n\n",
        "\n",
        "#predictions = model.predict(test_n)\n",
        "# show prediction results of all testing set"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sph3-cAkYfQB",
        "outputId": "196a4b2d-2e76-4ded-af8f-4578ff1cbc2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "\"\"\"\n",
        "NEW REFACTORED CODE\n",
        "\"\"\""
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nNEW REFACTORED CODE\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dsq-i0Sd3Nz"
      },
      "source": [
        ""
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ul9ED8QeeE7T"
      },
      "source": [
        "\n"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vz_spYuaBia2"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class OptimizersManager:\n",
        "  def __init__ (self):\n",
        "    self.__optimizerArr = np.array([\n",
        "      'SGD',\n",
        "      'RMSprop',\n",
        "      'Adam',\n",
        "      'Adadelta',\n",
        "      'Adagrad',\n",
        "      'Adamax',\n",
        "      'Nadam',\n",
        "      'Ftrl'\n",
        "    ])\n",
        "  \n",
        "  def get_optimizerArr(self) -> np.ndarray:\n",
        "    return self.__optimizerArr\n",
        "\n"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQTzs4yI5EPw"
      },
      "source": [
        "class ActivationFunctionsManager:\n",
        "  def __init__(self):\n",
        "    self.__activationFunctionArr = np.array([\n",
        "      'Relu'\n",
        "      ,\n",
        "      'Sigmoid'\n",
        "      ,\n",
        "      'Softmax'\n",
        "      ,\n",
        "      'Softplus'\n",
        "      ,\n",
        "      'Tanh'\n",
        "      ,\n",
        "      'Selu'\n",
        "      ,\n",
        "      'Elu'\n",
        "      ,\n",
        "      'Exponential'                \n",
        "    ])\n",
        "\n",
        "  def get_activationFunctionArr(self) -> np.ndarray:\n",
        "    return self.__activationFunctionArr"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2K1Zm6L4qcW"
      },
      "source": [
        "class Input_Shape_Manager:\n",
        "  def __init__ (self) -> None:\n",
        "    self.__url_name_to_input_shape = {\n",
        "        \"newUrl\" : [18,],\n",
        "        \"originalUrl\" : [19,]\n",
        "    }\n",
        "  \n",
        "  def get_input_shape(self, urlName : str) -> list:\n",
        "    return self.__url_name_to_input_shape[urlName]\n"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMrxOB_U6ilY"
      },
      "source": [
        "def compile(optimizer, metricVal):\n",
        "  try:\n",
        "        model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss='mape', metrics=[mape, 'mae'])\n",
        "        return True\n",
        "  except Exception as e:\n",
        "    print(\"ERROR: \")\n",
        "    print(e)\n",
        "    return False\n"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rk3Oa98dJOoQ"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "import numpy as np\n",
        "\n",
        "rmse = tf.keras.metrics.RootMeanSquaredError()\n",
        "\n",
        "mape = tf.keras.losses.MeanAbsolutePercentageError()\n",
        "\n",
        "class ModelMaker:\n",
        "  def __init__ (self,inputShape: list,  neuronCnt=64, activationFunction = \"relu\", optimizer = \"RMSprop\", metrics = [mape, 'mae'], comiplationDone = False, seedOfExecutionResult = 0 ):\n",
        "    self.__neuronCnt = neuronCnt\n",
        "    self.__compilationDone = False\n",
        "    self.__activationFunction = activationFunction\n",
        "    self.__seedOfExecutionResult =  seedOfExecutionResult\n",
        "    self.__optimizer = optimizer\n",
        "    self.__metrics = metrics\n",
        "    self.__model = None\n",
        "    self.__inputShape=inputShape\n",
        "\n",
        "\n",
        "  def makeNewModel(self):\n",
        "\n",
        "    # clear any previous model\n",
        "    self.__clearAllModels()\n",
        "    # generate a seed for replication purposes\n",
        "    self.__generateSeedForModel()\n",
        "\n",
        "\n",
        "    # notice input shape accommodates 18 features!\n",
        "    self.__model = Sequential([\n",
        "      Dense(self.__neuronCnt, activation=self.__activationFunction, input_shape=self.__inputShape),#was 18\n",
        "      Dense(self.__neuronCnt, activation=self.__activationFunction),\n",
        "      Dense(1)\n",
        "    ])\n",
        "\n",
        "    self.__compile()\n",
        "    return self.__model\n",
        "  \n",
        "  def __clearAllModels(self):\n",
        "    tf.keras.backend.clear_session()\n",
        "  \n",
        "  def __generateSeedForModel(self):\n",
        "    np.random.seed(self.__seedOfExecutionResult)\n",
        "    tf.random.set_seed(self.__seedOfExecutionResult)\n",
        "  \n",
        "  \n",
        "  def set_activationFunction (self, activationFunction = \"relu\"):\n",
        "    self.activationFunction = activationFunction\n",
        "\n",
        "  def set_seedOfExecutionResult (self, seedNum = 0):\n",
        "    self._seedOfExecutionResult = seedNum \n",
        "\n",
        "  def set_optimizer(self, optimizer):\n",
        "    self.__optimizer = optimizer\n",
        "  \n",
        "  def __compile(self):\n",
        "      try:\n",
        "\n",
        "        self.__model.compile(\n",
        "        optimizer=self.__optimizer,\n",
        "        loss='mape', metrics = self.__metrics)\n",
        "\n",
        "        self.__compilationDone = True\n",
        "\n",
        "      except ValueError as e:\n",
        "        print(\"Value Error: A value was inputted in the compile method that isn't compatible for it's use in the method body. Most likely the optimizer wasn't compatible. The Run will continue\")\n",
        "        print(\"FULL ERROR BELOW\")\n",
        "        print(e)\n",
        "\n",
        "        self.__compilationDone = False\n",
        "  \n",
        "  def __isCompiled(self):\n",
        "    return self.__compilationDone\n",
        "\n",
        "  def get_model(self):\n",
        "    return self.__model\n",
        "    \n"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFW6F_Yuj69P"
      },
      "source": [
        "class ModelTrainer:\n",
        "  def __init__(self, model, validationData, trainingData, epochs = 1 ):\n",
        "    self.__model = model\n",
        "    self.__epochs = epochs\n",
        "    self.__validationData = validationData\n",
        "    self.__trainingData = trainingData\n",
        "  def train(self):\n",
        "    history = self.__model.fit(self.__trainingData, epochs=self.__epochs,\n",
        "                    validation_data=self.__validationData)\n",
        "    return history\n",
        "  \n",
        "  def set_model(self, model):\n",
        "    self.__model = model\n"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mL9lNn5DGrx"
      },
      "source": [
        "manager = ActivationFunctionsManager()\n",
        "activationFunctionArr = manager.get_activationFunctionArr()\n",
        "\n",
        "manager = OptimizersManager()\n",
        "optimizerArr = manager.get_optimizerArr()\n",
        "\n",
        "\n",
        "\n",
        "class HistoryCollectorOfModelCombinations:\n",
        "  def __init__(self, modelMakerObj : ModelMaker, modelTrainerObj : ModelTrainer, optimizers = optimizerArr, activationFunctions = activationFunctionArr ):\n",
        "    self.__optimizerArr = optimizers\n",
        "    \n",
        "    self.__activationFunctionArr = activationFunctions\n",
        "\n",
        "    self.__iterationsToSkipSet = set((\n",
        "        \"\"\"\n",
        "        FORMAT:\n",
        "        (activationName, Optmizer),\n",
        "        \"\"\"\n",
        "    ))\n",
        "\n",
        "    self.__activationFunctionsToSkipSet = set(\n",
        "\n",
        "    \"\"\"\n",
        "    FORMAT:\n",
        "    'NameOfFunc',\n",
        "    \"\"\"   \n",
        "    \n",
        "  )\n",
        "    self.__modelTrainer = modelTrainerObj\n",
        "    self.__modelMaker = modelMakerObj\n",
        "\n",
        "  def collectHistories(self):\n",
        "    self.__preprocessStep()\n",
        "    \n",
        "    globalGroupID = \"collectHistories\"\n",
        "\n",
        "    histories = []\n",
        "    \n",
        "    globalDL.print(\"ARR After preprocess:\\n\" + str(self.__activationFunctionArr), globalGroupID)\n",
        "\n",
        "    for activationFunction in self.__activationFunctionArr:\n",
        "\n",
        "\n",
        "      if(activationFunction in activationFunctionsToSkipSet):\n",
        "            continue\n",
        "\n",
        "      for optimizer in self.__optimizerArr:\n",
        "        activationFunctionToOptimizerTuple = (activationFunction, optimizer)\n",
        "        \n",
        "        globalDL.print(str(activationFunction) + str(optimizer) + \"\\n\", globalGroupID)\n",
        "\n",
        "        if(activationFunctionToOptimizerTuple in iterationsToSkipSet):\n",
        "          continue\n",
        "\n",
        "        histories.append(self.__makeHistory(activationFunction,optimizer))\n",
        "      \n",
        "    return histories\n",
        "\n",
        "  def __makeHistory(self, activationFunction, optimizer):\n",
        "    self.__modelMaker.set_activationFunction(activationFunction)\n",
        "    self.__modelMaker.set_optimizer(optimizer)\n",
        "\n",
        "    model = self.__modelMaker.makeNewModel()\n",
        "    \n",
        "    self.__modelTrainer.set_model(model)\n",
        "\n",
        "    history = self.__modelTrainer.train()\n",
        "\n",
        "    return history\n",
        "\n",
        "    \n",
        "\n",
        "  def __preprocessStep(self):\n",
        "    self.__activationFunctionArr = np.char.lower(self.__activationFunctionArr)\n"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iV0s6dMDFZNY"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class HistoryPlotter:\n",
        "  def __init__ (self, history=None, limit1=mean_absolute_percentage_error_limit, limit2=mae_limit):\n",
        "    self.__plt = plt\n",
        "    self.__limit1 = limit1\n",
        "    self.__limit2 = limit2 #NOT USED BUT PRESENT IN ORIGINAL (2021/11/3)\n",
        "    self.__history = history\n",
        "  def makePlot(self):\n",
        "    plt = self.__plt \n",
        "    history = self.__history\n",
        "    limit1 = self.__limit1\n",
        "\n",
        "    hist = pd.DataFrame(history.history)\n",
        "    hist['epoch'] = history.epoch\n",
        "    plt.figure()\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('MAPE [MPG]')\n",
        "    plt.plot(hist['epoch'], hist['mean_absolute_percentage_error'],    label='Train Error')\n",
        "    plt.plot(hist['epoch'], hist['val_mean_absolute_percentage_error'],    label = 'Val Error')\n",
        "    plt.ylim([0, limit1])\n",
        "    plt.legend()\n",
        "    plt.title('MAPE by Epoch')\n",
        "    \n",
        "    self.__plt = plt\n",
        "\n",
        "  def showPlot(self):\n",
        "    self.__plt.show()\n",
        "\n",
        "  def savePlot(self, strFileName : str):\n",
        "    self.__plt.savefig(strFileName)    \n",
        "\n",
        "  def set_history(self, history):\n",
        "    self.__history = history\n",
        "  \n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TestRunner:\n",
        "  def __init__(self) -> None:\n",
        "    self.__testClasses = {\n",
        "          0: TestConverterToTensorDatasetBusiness,\n",
        "          1: TestDatasetProcessor\n",
        "    }\n",
        "\n",
        "    self.__totalNumOfTestClasses = 2\n",
        "\n",
        "  def runOne(self, index :int):\n",
        "    testObj = self.__testClasses[index]()\n",
        "    testObj.run() \n",
        "\n",
        "  def runAll(self):\n",
        "    for index in range(0, self.__totalNumOfTestClasses):\n",
        "       self.runOne(index)\n"
      ],
      "metadata": {
        "id": "cy4iPwCK_oxj"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHqjTvwScvHn"
      },
      "source": [
        "def runUnitTests():\n",
        "  Tobj = TestSameHistoryDifferentSessions()\n",
        "  print(Tobj.isEqual())"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import date\n",
        "\n",
        "def runEachModelForEachSample():\n",
        "  epochs = 50\n",
        "  validationTargetValues = y_test\n",
        "  for sample in validationTargetValues:\n",
        "    modelMakerObj = ModelMaker()\n",
        "    model = modelMakerObj.makeNewModel()\n",
        "    modelTrainerObj = ModelTrainer(model, test_bs, train_bs, epochs)\n",
        "    history = modelTrainerObj.train()\n",
        "    obj = HistoryPlotter()\n",
        "    obj.set_history(history)\n",
        "    obj.makePlot()\n",
        "    name = f\"Data_time:{date.today()}_epochs:{epochs}_.png\"\n",
        "    obj.savePlot(name)\n",
        "    files.download(name)\n"
      ],
      "metadata": {
        "id": "_4YF-3GpLn_U"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "166jHy352IWL"
      },
      "source": [
        "from google.colab import files #for downloading browser\n",
        "def runOne():\n",
        "  epochs = 10000\n",
        "  modelMakerObj = ModelMaker()\n",
        "  model = modelMakerObj.makeNewModel()\n",
        "  modelTrainerObj = ModelTrainer(model, test_bs, train_bs, epochs)\n",
        "  history = modelTrainerObj.train()\n",
        "  obj = HistoryPlotter()\n",
        "  obj.set_history(history)\n",
        "  obj.makePlot()\n",
        "  name = \"newData_10000_epochs.png\"\n",
        "  obj.savePlot(name)\n",
        "  files.download(name)"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75ox1PJwwOju"
      },
      "source": [
        "def getHistoriesOfAllModelCombinations():\n",
        "  modelMakerObj = ModelMaker()\n",
        "  model = modelMakerObj.makeNewModel()\n",
        "  modelTrainerObj = ModelTrainer(model, test_bs, train_bs, 1)\n",
        "\n",
        "  HistoryCollectorObj = HistoryCollectorOfModelCombinations(modelMakerObj, modelTrainerObj)\n",
        "  histories = HistoryCollectorObj.collectHistories()\n",
        "\n",
        "  return histories\n",
        "\n",
        "\n"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oxmj89vDGjGu"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yn467D6CJJ3P"
      },
      "source": [
        "def makePlotNames():\n",
        "  plotNames = [ ]\n",
        "  for activationFunction in activationFunctionArr:\n",
        "    if(activationFunction in activationFunctionsToSkipSet):\n",
        "          continue\n",
        "\n",
        "    for optimizer in optimizerArr:\n",
        "      activationFunctionToOptimizerTuple = (activationFunction, optimizer)\n",
        "      \n",
        "      plotNames.append(\"ActivationFunction : \" + str(activationFunction) + \", Optimizer: \" + str(optimizer))\n",
        "  \n",
        "      if(activationFunctionToOptimizerTuple in iterationsToSkipSet):\n",
        "        continue\n",
        "\n",
        "  return plotNames \n"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-Z8GxN_zvWo"
      },
      "source": [
        "def plotHistories( historiesContainer: list , nameOfFigures : list) -> None:\n",
        "  historyPlotterObj = HistoryPlotter()\n",
        "  nameOfFiguresSz = len(nameOfFigures)\n",
        "  nameOfFiguresIndex = 0\n",
        "\n",
        "  for history in historiesContainer:\n",
        "\n",
        "    historyPlotterObj.set_history(history)\n",
        "    historyPlotterObj.makePlot()\n",
        "    historyPlotterObj.showPlot()\n",
        "    historyPlotterObj.savePlot(nameOfFigures[nameOfFiguresIndex]+\".png\")\n",
        "    if(nameOfFiguresIndex < nameOfFiguresSz - 1): #\n",
        "      nameOfFiguresIndex += 1\n",
        "\n",
        "    "
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KMq3LMTsGE3"
      },
      "source": [
        "class ModelPredictor:\n",
        "  def __init__ (self, y_test, inputSample,model):\n",
        "    self.__inputSample = test_n\n",
        "    self.__model = model\n",
        "  def predict(self):\n",
        "    #Tensorflow recommends __call__ to be used for \"smaller\" batch sizes\n",
        "    return self.__model.predict(self.__inputSample)"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfnyt2xjND_e"
      },
      "source": [
        "from google.colab import files #for downloading browser\n",
        "def downloadAllFigures(nameOfFigures):\n",
        "  for name in nameOfFigures:\n",
        "    files.download(name+\".png\")"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-47rl7SuEUw"
      },
      "source": [
        "class PredictorResultsView:\n",
        "  def __init__(self, testingSetLabels, predictions):\n",
        "    self.__eight = predictions[:8]\n",
        "    self.__actuals = testingSetLabels[:8]\n",
        "  def showPredictions(self):\n",
        "    eight = self.__eight\n",
        "    actuals = self.__actuals \n",
        "    eight = predictions[:8]\n",
        "    actuals = testingSetLabels[:8]\n",
        "    print ('pred', 'actual')\n",
        "    for i, p in enumerate(range(8)):\n",
        "      print (np.round(eight[i][0],1), actuals[i])\n",
        "\n"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uu1GQWvo8EGk"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "#optimizerArr = np.char.lower(optimizerArr)\n",
        "def original_run_all_combinations_code(activationFunctionArr : np.array, activationFunctionsToSkipSet: set, iterationsToSkipSet : set ):\n",
        "  activationFunctionArr = np.char.lower(activationFunctionArr)\n",
        "\n",
        "\n",
        "  for activationFunction in activationFunctionArr:\n",
        "    if(activationFunction in activationFunctionsToSkipSet):\n",
        "          continue\n",
        "\n",
        "    for optimizer in optimizerArr:\n",
        "      activationFunctionToOptimizerTuple = (activationFunction, optimizer)\n",
        "\n",
        "      if(activationFunctionToOptimizerTuple in iterationsToSkipSet):\n",
        "        continue\n",
        "      #NOTE: need to encapsulate in try/catch\n",
        "      #keras\n",
        "      tf.keras.backend.clear_session()\n",
        "\n",
        "      # generate a seed for replication purposes\n",
        "      np.random.seed(0)\n",
        "      tf.random.set_seed(0)\n",
        "\n",
        "      # notice input shape accommodates 18 features!\n",
        "      #error: Relu =/= relu (correct)\n",
        "      model = Sequential([\n",
        "        Dense(64, activation=activationFunction, input_shape=[18,]),\n",
        "        Dense(64, activation=activationFunction),\n",
        "        Dense(1)\n",
        "      ])\n",
        "\n",
        "    \n",
        "\n",
        "      rmse = tf.keras.metrics.RootMeanSquaredError()\n",
        "\n",
        "      mape = tf.keras.losses.MeanAbsolutePercentageError()\n",
        "\n",
        "      print(\"acitvationFunc \" + activationFunction)\n",
        "      print(\"optimizer \" + optimizer)\n",
        "      isCompilationDone = compile(optimizer, mape)\n",
        "      if(isCompilationDone):\n",
        "        print(\"compileIsDone\")\n",
        "        history = model.fit(train_bs, epochs=20,\n",
        "                      validation_data=test_bs)\n",
        "  # create a model with one input layer, one hidden layer, and on outpur layer. Both input and hidden layers have 64 neuros. Activation using Relu.\n",
        "      #plot results \n",
        "        plt = plot_history(history, mean_absolute_percentage_error_limit, mae_limit)\n",
        "        fileNameStr=activationFunction+optimizer+\".png\"\n",
        "    #export\n",
        "        plt.savefig(fileNameStr)\n",
        "\n"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6IFkJW_IQJM"
      },
      "source": [
        "import tensorflow as tf \n",
        "\n",
        "def original_train_model(model : tf.keras.Sequential, test_bs : tf.data.Dataset) -> tf.keras.callbacks.History:\n",
        "  history = model.fit(train_bs, epochs=2000,\n",
        "                      validation_data=test_bs)\n",
        "  return history\n",
        "# epoch ( iteration of training) can start from 50\n",
        "# about 10 ms for each epoch, keep watching the loss value (for traqining set) and val_loss value ( for testing set)"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-JnxDb5cqrR"
      },
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "def original_show_last_five_epoch(history:  tf.keras.callbacks.History) -> pd.DataFrame:\n",
        "  hist = pd.DataFrame(history.history)\n",
        "  hist['epoch'] = history.epoch\n",
        "  hist.head()\n",
        "  print\n",
        "  hist.tail()\n",
        "  return hist\n",
        "# show the result of last five epoch"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4v2WNLGn0go9"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def original_compile(model: tf.keras.Sequential) -> tf.keras.Sequential:\n",
        "\n",
        "  rmse = tf.keras.metrics.RootMeanSquaredError()\n",
        "\n",
        "  mape = tf.keras.losses.MeanAbsolutePercentageError()\n",
        "\n",
        "  model.compile(\n",
        "      optimizer='RMSProp',\n",
        "      loss='mape', metrics=[mape, 'mae'])\n",
        "  \n",
        "  return model\n",
        "\n",
        "# use mape (mean absolute percentage error) as loss function"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guR97iNB8WaY"
      },
      "source": [
        ""
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kF070EKn0VHW"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def original_model_summary(model : tf.keras.Sequential ) -> None:\n",
        "  model.summary()\n",
        "# number of params in input layer \"dense\" = (number of feaures +1) * number of neuros = (18+1) * 64 = 1216\n",
        "# number of params in hidden layer \"dense_1\" = (number of outputs from input layer +1) * number of neuros = (64+1) * 64 = 4196\n",
        "# number of params in ouput layer \"dense2\" = (number of outputs from hidden layer +1) * number of neuros = (64+1) * 1 = 65"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f92pOalK0OZu"
      },
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "import numpy as np\n",
        "\n",
        "def clear_any_previous_model():\n",
        "  tf.keras.backend.clear_session()\n",
        "\n",
        "# generate a seed for replication purposes\n",
        "def original_generate_a_seed_for_replication_purposes():\n",
        "  np.random.seed(0)\n",
        "  tf.random.set_seed(0)\n",
        "\n",
        "# notice input shape accommodates 18 features!\n",
        "\n",
        "def original_create_model():\n",
        "  model = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=[18,]),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1)\n",
        "  ])\n",
        "# create a model with one input layer, one hidden layer, and on outpur layer. Both input and hidden layers have 64 neuros. Activation using Relu."
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcheS1pYcW3x"
      },
      "source": [
        "\n",
        "class TestSameHistoryDifferentSessions(unittest.TestCase):\n",
        "  def __init__ (self):\n",
        "    self.__epochs = 5\n",
        "    self.__expectedHistory = self.__getHistoryFromGivenCode()\n",
        "    self.__experimentHistory = self.__getHistoryFromGivenCode()\n",
        "\n",
        "\n",
        "  # code doesn't work\n",
        "  def __getHistoryFromGivenCode(self):\n",
        "    import tensorflow as tf\n",
        "    from tensorflow.keras.models import Sequential\n",
        "    from tensorflow.keras.layers import Dense, Flatten\n",
        "    import numpy as np\n",
        "\n",
        "    # clear any previous model\n",
        "    tf.keras.backend.clear_session()\n",
        "\n",
        "    # generate a seed for replication purposes\n",
        "    np.random.seed(0)\n",
        "    tf.random.set_seed(0)\n",
        "\n",
        "    # notice input shape accommodates 18 features!\n",
        "    model = Sequential([\n",
        "      Dense(64, activation='relu', input_shape=global_input_shape),#shape was 18\n",
        "      Dense(64, activation='relu'),\n",
        "      Dense(1)\n",
        "    ])\n",
        "    # create a model with one input layer, one hidden layer, and on outpur layer. Both input and hidden layers have 64 neuros. Activation using Relu.\n",
        "    rmse = tf.keras.metrics.RootMeanSquaredError()\n",
        "\n",
        "    mape = tf.keras.losses.MeanAbsolutePercentageError()\n",
        "\n",
        "\n",
        "\n",
        "    model.compile(\n",
        "        optimizer='RMSProp',\n",
        "        loss='mape', metrics=[mape, 'mae'])\n",
        "    \n",
        "    #given by above code\n",
        "    history = model.fit(train_bs, epochs = self.__epochs,\n",
        "                    validation_data=test_bs)\n",
        "    # epoch ( iteration of training) can start from 50\n",
        "    # about 10 ms for each epoch, keep watching the loss value (for training set) and val_loss value ( for testing set)\n",
        "\n",
        "    tf.keras.backend.clear_session()\n",
        "\n",
        "    return history\n",
        "      \n",
        "  def isEqual(self):\n",
        "    #NOTE: Equality may be too exact. I need to understand how much variance of \"loss\" values is expected between two different model runs. The model run seems to be dependent on library imports (the objects)\n",
        "    # because that removed the \"continuation\" pattern. As of 2021/11/4, the result is False although some values from the histories overlap\n",
        "    return (self.__expectedHistory == self.__experimentHistory)\n",
        "  def test(self):\n",
        "    self.assertEqual(self.__experimentHistory, self.__expectedHistory)\n",
        "    #Should pause program if fails"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9gD42S66AiQ"
      },
      "source": [
        "\n",
        "#test is broken right now because of inherit problem.\n",
        "def runAllUnitTests():\n",
        "  tObj = TestHistoryFromGivenCode()\n",
        "  tObj.test()\n",
        "\n"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kju3zvz725tQ"
      },
      "source": [
        "import unittest\n",
        "\n",
        "class TestHistoryFromGivenCode(unittest.TestCase):\n",
        "  def __init__ (self):\n",
        "    self.__epochs = 5\n",
        "    self.__expectedHistory = self.__getHistoryFromGivenCode()\n",
        "    self.__experimentHistory = self.__getHistoryFromRefactoredCode()\n",
        "\n",
        "\n",
        "  # code doesn't work\n",
        "  def __getHistoryFromGivenCode(self):\n",
        "    import tensorflow as tf\n",
        "    from tensorflow.keras.models import Sequential\n",
        "    from tensorflow.keras.layers import Dense, Flatten\n",
        "    import numpy as np\n",
        "\n",
        "    # clear any previous model\n",
        "    tf.keras.backend.clear_session()\n",
        "\n",
        "    # generate a seed for replication purposes\n",
        "    np.random.seed(0)\n",
        "    tf.random.set_seed(0)\n",
        "\n",
        "    # notice input shape accommodates 18 features!\n",
        "    model = Sequential([\n",
        "      Dense(64, activation='relu', input_shape=global_input_shape),#shape was 18\n",
        "      Dense(64, activation='relu'),\n",
        "      Dense(1)\n",
        "    ])\n",
        "    # create a model with one input layer, one hidden layer, and on outpur layer. Both input and hidden layers have 64 neuros. Activation using Relu.\n",
        "    rmse = tf.keras.metrics.RootMeanSquaredError()\n",
        "\n",
        "    mape = tf.keras.losses.MeanAbsolutePercentageError()\n",
        "\n",
        "\n",
        "\n",
        "    model.compile(\n",
        "        optimizer='RMSProp',\n",
        "        loss='mape', metrics=[mape, 'mae'])\n",
        "    \n",
        "    #given by above code\n",
        "    history = model.fit(train_bs, epochs = self.__epochs,\n",
        "                    validation_data=test_bs)\n",
        "    # epoch ( iteration of training) can start from 50\n",
        "    # about 10 ms for each epoch, keep watching the loss value (for training set) and val_loss value ( for testing set)\n",
        "\n",
        "    tf.keras.backend.clear_session()\n",
        "\n",
        "    return history\n",
        "\n",
        "  def __getHistoryFromRefactoredCode(self):\n",
        "    import tensorflow as tf\n",
        "    from tensorflow.keras.models import Sequential\n",
        "    from tensorflow.keras.layers import Dense, Flatten\n",
        "    import numpy as np\n",
        "\n",
        "    rmse = tf.keras.metrics.RootMeanSquaredError()\n",
        "\n",
        "    mape = tf.keras.losses.MeanAbsolutePercentageError()\n",
        "\n",
        "    modelMakerObj = ModelMaker()\n",
        "    model = modelMakerObj.makeNewModel()\n",
        "    modelTrainerObj = ModelTrainer(model, test_bs, train_bs, self.__epochs)\n",
        "    return modelTrainerObj.train()\n",
        "\n",
        "      \n",
        "  def isEqual(self):\n",
        "    #NOTE: Equality may be too exact. I need to understand how much variance of \"loss\" values is expected between two different model runs. The model run seems to be dependent on library imports (the objects)\n",
        "    # because that removed the \"continuation\" pattern. As of 2021/11/4, the result is False although some values from the histories overlap\n",
        "    return (self.__expectedHistory == self.__experimentHistory)\n",
        "  def test(self):\n",
        "    self.assertEqual(self.__experimentHistory, self.__expectedHistory)\n",
        "    #Should pause program if fails\n",
        "  \n",
        "      \n",
        "\n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def testRunnerExec():\n",
        "  testRunnerObj = TestRunner()\n",
        "  testRunnerObj.runOne(1)\n",
        "  testRunnerObj.runAll()"
      ],
      "metadata": {
        "id": "FFDxLPYCCVIi"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "class InvidualTestSampleExecutor:\n",
        "  def __init__(self):\n",
        "    pass\n",
        "  \n",
        "  def main(self):\n",
        "    url = self.get_raw_data()\n",
        "\n",
        "  def get_raw_data(self)  -> pd.DataFrame:\n",
        "    manager = UrlManager()\n",
        "    url = manager.get_newDataUrl()\n",
        "    return original_read_csv(url)\n",
        "    \n",
        "  def get_raw_model_dataset(self, raw_data : pd.DataFrame ) ->  dict:\n",
        "    manager = UrlManager()\n",
        "    url = manager.get_newDataUrl()\n",
        "    urlName = \"newDataUrl\"\n",
        "    attributeToTargetData = choose_attribute_name(urlName)\n",
        "    print(attributeToTargetData)\n",
        "    print(raw_data.columns)\n",
        "\n",
        "    target = raw_data.pop(attributeToTargetData)\n",
        "\n",
        "    creator = InputVariablesCreator(raw_data)\n",
        "    \n",
        "    input_manager = creator.seperate_training_and_testing_inputs()\n",
        "    \n",
        "    creator = TargetVariablesCreator(target)\n",
        "    \n",
        "    target_manager = creator.seperate_training_and_testing_targets()\n",
        "\n",
        "\n",
        "    raw_data_manager = {\n",
        "        \"input_manager\" : input_manager,\n",
        "        \"target_manager\" : target_manager\n",
        "    }\n",
        "\n",
        "    print(input_manager)\n",
        "\n",
        "    return raw_data_manager\n",
        "\n",
        "\n",
        "  def create_tensor_dataset(self, raw_data_manager: dict)-> dict:  \n",
        "    keyToInputManager = \"input_manager\"\n",
        "    input_manager = raw_data_manager[keyToInputManager]\n",
        "\n",
        "    keyToTargetManager  = \"target_manager\"\n",
        "    target_manager = raw_data_manager[keyToTargetManager]\n",
        "\n",
        "    training_input_data = input_manager.get_train_input_variables()\n",
        "    training_target_data = target_manager.get_train_target_variables() \n",
        "\n",
        "\n",
        "    training_dataset = TrainingDataConverterToTensorDatasetBusiness(training_target_data, training_input_data)\n",
        "\n",
        "\n",
        "    validation_input_data = input_manager.get_validation_input_variables()\n",
        "    validation_target_data = target_manager.get_validation_target_variables()\n",
        "\n",
        "    multiple_validation_datasets = self.__make_each_testing_sample_a_dataset(validation_input_data , validation_target_data)\n",
        "\n",
        "    datasets = {\n",
        "        \"train\": training_dataset,\n",
        "        \"test\" : multiple_validation_datasets\n",
        "    }\n",
        "\n",
        "    return datasets\n",
        "  \n",
        "  def __make_each_testing_sample_a_dataset(self, validation_input_data : list, validation_target_data : list) -> list:\n",
        "      multiple_validation_datasets = []\n",
        "      num_of_validation_tuples = len(validation_target_data)\n",
        "\n",
        "      print(f\"input: {validation_input_data}\")\n",
        "      print(f\"target: {validation_target_data}\")\n",
        "\n",
        "\n",
        "      for index in range(num_of_validation_tuples) :\n",
        "        input_data_for_current_target_data = validation_input_data[index]\n",
        "        current_target_data = validation_target_data[index]\n",
        "\n",
        "        print(f\"input: {input_data_for_current_target_data}\")\n",
        "        print(f\"target: {current_target_data}\")\n",
        "        converter = ValidationDataConvertToTensorDatasetBusiness(input_data_for_current_target_data, current_target_data)\n",
        "        validation_dataset = converter.convert()\n",
        "        multiple_validation_datasets.append(validation_dataset)\n",
        "\n",
        "      return multiple_validation_datasets\n",
        "        \n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "  \n",
        "  def process_tensor_dataset(self, tensor_dataset: dict) -> dict:\n",
        "    keyToTrainingDataset = \"train\"\n",
        "    training_dataset = tensor_dataset[keyToTrainingDataset]\n",
        "\n",
        "    processor = TrainingDatasetProcessor(training_dataset)\n",
        "    processed_training_dataset = processor.process_data()\n",
        "\n",
        "    keyToValidationDataset = \"test\"\n",
        "    multiple_validation_datasets = tensor_dataset[keyToValidationDataset]\n",
        "\n",
        "    multiple_processed_validation_datasets = self.__process_each_validation_dataset(multiple_validation_datasets)\n",
        "\n",
        "\n",
        "    processed_datasets = {\n",
        "        \"train\" : processed_training_dataset,\n",
        "        \"test\" : multiple_processed_validation_datasets\n",
        "    }\n",
        "\n",
        "\n",
        "    print(processed_datasets)\n",
        "\n",
        "    return processed_datasets\n",
        "  \n",
        "  def __process_each_validation_dataset(self, multiple_validation_datasets : list) -> list:\n",
        "    multiple_processed_datasets = []\n",
        "    for dataset in  multiple_validation_datasets:\n",
        "      processor = ValiationDatasetProcessor(dataset)\n",
        "      processed_validation_dataset = processor.process_data()\n",
        "      multiple_processed_datasets.append(processed_validation_dataset)\n",
        "    \n",
        "    return multiple_processed_datasets\n",
        "\n",
        "      \n",
        "\n",
        "  def train_model(self):\n",
        "    model = self.__create_model()\n",
        "    print(model)\n",
        "\n",
        "    trainer = ModelTrainer()\n",
        "     \n",
        "  def __create_model(self: dict)-> tf.keras.Model:\n",
        "    manager = Input_Shape_Manager()\n",
        "    urlName = \"newUrl\"\n",
        "    input_shape = manager.get_input_shape(urlName)\n",
        "    maker = ModelMaker(input_shape)\n",
        "    model = maker.makeNewModel()\n",
        "    return model\n",
        "\n",
        "       "
      ],
      "metadata": {
        "id": "KniwbejPqqN2"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "executor = InvidualTestSampleExecutor()"
      ],
      "metadata": {
        "id": "Vp9ZF9GgtIVG"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_data = executor.get_raw_data()"
      ],
      "metadata": {
        "id": "cgykuebetPgs"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_data_manager = executor.get_raw_model_dataset(raw_data)"
      ],
      "metadata": {
        "id": "r2_XhJR9t1ip",
        "outputId": "d54e94b1-1242-4ceb-af10-5c7ac5b0e5a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "adjusted execution time\n",
            "Index(['Number of statements', 'No. of  iterations in outer',\n",
            "       'No. of  iterations in inner', 'No. of statements in outer ',\n",
            "       'No. of statements in inner', 'Number of loops',\n",
            "       'levels of nested loops', 'total  iterations ',\n",
            "       '# of Complex Variables', 'CC*', 'function calls', 'branches ', 'SWPP',\n",
            "       'Number of inputs', 'Number of outputs', 'Sizes of input data (byte)',\n",
            "       'Sizes of output data (byte)', 'No. 0f iterations* statement inner',\n",
            "       'repeating time', 'adjusted execution time'],\n",
            "      dtype='object')\n",
            "<__main__.InputVariablesManager object at 0x7f4bb670ecd0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datasets = executor.create_tensor_dataset(raw_data_manager)\n",
        "print(datasets)"
      ],
      "metadata": {
        "id": "XO5tHNnazQQw",
        "outputId": "a810414b-dc2a-42da-84c3-24721b77ebd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 918
        }
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input:     Number of statements  ...  repeating time\n",
            "25              0.030864  ...           0.004\n",
            "26              0.080247  ...           0.001\n",
            "27              0.216049  ...           0.006\n",
            "28              0.030864  ...           0.042\n",
            "29              0.203704  ...           0.006\n",
            "30              0.148148  ...           0.001\n",
            "31              0.339506  ...           0.169\n",
            "32              0.135802  ...           0.102\n",
            "\n",
            "[8 rows x 19 columns]\n",
            "target: 25      281724\n",
            "26    10374917\n",
            "27      665532\n",
            "28      143766\n",
            "29      659850\n",
            "30      479246\n",
            "31      531167\n",
            "32      160038\n",
            "Name: adjusted execution time, dtype: int64\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2897\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2898\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 0",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-103-36b287a3524d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdatasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_tensor_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_data_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-95-0dc13c457cf1>\u001b[0m in \u001b[0;36mcreate_tensor_dataset\u001b[0;34m(self, raw_data_manager)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mvalidation_target_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_validation_target_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0mmultiple_validation_datasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__make_each_testing_sample_a_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_input_data\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mvalidation_target_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     datasets = {\n",
            "\u001b[0;32m<ipython-input-95-0dc13c457cf1>\u001b[0m in \u001b[0;36m__make_each_testing_sample_a_dataset\u001b[0;34m(self, validation_input_data, validation_target_data)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_of_validation_tuples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0minput_data_for_current_target_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation_input_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0mcurrent_target_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation_target_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2904\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2905\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2906\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2907\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2908\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2898\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2900\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2902\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 0"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "processed_datasets = executor.process_tensor_dataset(datasets)"
      ],
      "metadata": {
        "id": "rSYnBCbK08nw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "executor.train_model()"
      ],
      "metadata": {
        "id": "H9SLLpsA5Qc8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}